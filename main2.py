from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
import streamlit as st
from dotenv import load_dotenv
import os
import time
from utils.env_util import load_env_vars

OPENAI_API_KEY = None
OPENAI_BASE_URL = None
MODEL = None

# ä»…é¦–æ¬¡æ‰§è¡Œ
env_vars = load_env_vars()

with st.sidebar:
    env_vars["CURRENT_MODEL"] = st.selectbox(
        label="é€‰æ‹©æ¨¡å‹",
        options=env_vars["MODEL_LIST"],
        index=0,
        help="é€‰æ‹© LLM æ¨¡å‹çš„ç§ç±»",
    )
    # æ»‘åŠ¨æ¡
    env_vars['TEMPERATURE'] = st.slider(
        label="Temperature",
        min_value=0.0, max_value=1.0, value=0.8,
        help="Temperature å‚æ•°ç”¨äºæ§åˆ¶ LLM çš„è¾“å‡ºå¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œé«˜ Temperature å¢åŠ å¤šæ ·æ€§ä½†å¯èƒ½é™ä½ç¡®å®šæ€§ï¼Œä½ Temperature åˆ™å¢åŠ ç¡®å®šæ€§ä½†å¯èƒ½é™ä½å¤šæ ·æ€§ã€‚"
    )


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨ä¸äººç±»å¯¹è¯çš„AIèŠå¤©æœºå™¨äºº"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(
    openai_api_key=env_vars["OPENAI_API_KEY"], 
    model_name=env_vars["CURRENT_MODEL"], 
    temperature=env_vars["TEMPERATURE"], 
    base_url=env_vars["OPENAI_BASE_URL"], 
)
chain = prompt | llm
chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: msgs,
    input_messages_key="question",
    history_messages_key="history",
)


view_messages = st.expander("æŸ¥çœ‹ä¼šè¯çŠ¶æ€ä¸­çš„æ¶ˆæ¯å†…å®¹")

# StreamlitChatMessageHistory åˆå§‹åŒ–ï¼ˆkey ç”¨äºåœ¨ session_state ä¸­å”¯ä¸€æ ‡è¯†å­˜å‚¨ï¼‰
msgs = StreamlitChatMessageHistory(key="langchain_messages")
if len(msgs.messages) == 0:
    msgs.add_ai_message("ğŸ™ƒ æœ‰é—®é¢˜å¯ä»¥å‘æˆ‘æé—®å“¦~")

# ä» StreamlitChatMessageHistory æ¸²æŸ“å½“å‰æ¶ˆæ¯ï¼Œè‡ªåŠ¨åŒºåˆ†è§’è‰² human å’Œ ai æ¶ˆæ¯
for msg in msgs.messages:
    st.chat_message(msg.type).write(msg.content)

# å®šä¹‰ä¸€ä¸ªç”¨æˆ·è¾“å…¥æ¡†å¹¶å¤„ç†ç›¸å…³é€»è¾‘
if prompt := st.chat_input(placeholder = "shift+enter æ¢è¡Œ", max_chars = 2048):
    st.chat_message("human").write(prompt)
    # å½“ä½¿ç”¨ â€‹â€‹StreamlitChatMessageHistoryâ€‹â€‹ æ—¶ï¼Œæ–°æ¶ˆæ¯ä¼šåœ¨è¿è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¿å­˜åˆ°å†å²è®°å½•ä¸­ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†
    config = {"configurable": {"session_id": "any"}}
    response = chain_with_history.invoke({"question": prompt}, config)
    st.chat_message("ai").write(response.content)

# åœ¨æœ€åç»˜åˆ¶æ¶ˆæ¯ï¼Œä»¥ä¾¿æ–°ç”Ÿæˆçš„æ¶ˆæ¯ç«‹å³æ˜¾ç¤º
with view_messages:
    view_messages.json(st.session_state.langchain_messages)
