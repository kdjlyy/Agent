import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.messages import AIMessageChunk
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI

from utils.envs_util import load_env_vars

OPENAI_API_KEY = None
OPENAI_BASE_URL = None
MODEL = None

# ä»…é¦–æ¬¡æ‰§è¡Œ
env_vars = load_env_vars()

with st.sidebar:
    env_vars["CURRENT_MODEL"] = st.selectbox(
        label="é€‰æ‹©æ¨¡å‹",
        options=env_vars["MODEL_LIST"],
        index=0,
        help="é€‰æ‹© LLM æ¨¡å‹çš„ç§ç±»",
    )
    # æ»‘åŠ¨æ¡
    env_vars['TEMPERATURE'] = st.slider(
        label="Temperature",
        min_value=0.0, max_value=1.0, value=0.8,
        help="Temperature å‚æ•°ç”¨äºæ§åˆ¶ LLM çš„è¾“å‡ºå¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œé«˜ Temperature å¢åŠ å¤šæ ·æ€§ä½†å¯èƒ½é™ä½ç¡®å®šæ€§ï¼Œä½ Temperature åˆ™å¢åŠ ç¡®å®šæ€§ä½†å¯èƒ½é™ä½å¤šæ ·æ€§ã€‚"
    )

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "ä½ æ˜¯ä¸€ä¸ªæ­£åœ¨ä¸äººç±»å¯¹è¯çš„AIèŠå¤©æœºå™¨äºº"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(
    openai_api_key=env_vars["OPENAI_API_KEY"], 
    model_name=env_vars["CURRENT_MODEL"], 
    temperature=env_vars["TEMPERATURE"], 
    base_url=env_vars["OPENAI_BASE_URL"], 
    streaming=True,
)
chain = prompt | llm
chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: msgs,
    input_messages_key="question", # è¾“å…¥å˜é‡å
    history_messages_key="history", # å†å²å ä½ç¬¦å˜é‡å
)

# StreamlitChatMessageHistory åˆå§‹åŒ–ï¼ˆkey ç”¨äºåœ¨ session_state ä¸­å”¯ä¸€æ ‡è¯†å­˜å‚¨ï¼‰
msgs = StreamlitChatMessageHistory(key="langchain_messages")
if len(msgs.messages) == 0:
    msgs.add_ai_message("ğŸ™ƒ æœ‰é—®é¢˜å¯ä»¥å‘æˆ‘æé—®å“¦~")

# ä» StreamlitChatMessageHistory æ¸²æŸ“å½“å‰æ¶ˆæ¯ï¼Œè‡ªåŠ¨åŒºåˆ†è§’è‰² human å’Œ ai æ¶ˆæ¯
for msg in msgs.messages:
    st.chat_message(msg.type).write(msg.content)

# ç”¨æˆ·è¾“å…¥å¤„ç†é€»è¾‘
if prompt := st.chat_input():
    st.chat_message("human").write(prompt)
    config = {"configurable": {"session_id": "user_123"}}
    
    # åˆ›å»ºæ¶ˆæ¯å®¹å™¨å¹¶åˆå§‹åŒ–
    with st.chat_message("ai"):
        response_container = st.empty()
        full_response = ""
        # æµå¼è·å–å“åº”
        for chunk in chain_with_history.stream({"question": prompt}, config):
            if isinstance(chunk, AIMessageChunk):
                full_response += chunk.content
                response_container.markdown(full_response)
        # å°†å®Œæ•´å“åº”åŠ å…¥å†å²
        # msgs.add_ai_message(full_response)  

