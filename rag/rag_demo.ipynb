{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_number=None, min_line_length=1):\n",
    "    \"\"\"Extract text from a PDF file.\n",
    "    Args:\n",
    "        filename (str): Path to the PDF file.\n",
    "        page_number (int, optional): Page number to extract. If None, extracts all pages.\n",
    "        min_line_length (int, optional): Minimum length of a line to be included in the output.\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    buffer, full_text = '', ''\n",
    "    # æå–å…¨éƒ¨æ–‡æœ¬\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        if page_number is not None and i != page_number:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # æŒ‰ç©ºè¡Œåˆ†éš”ï¼Œå°†æ–‡æœ¬é‡æ–°ç»„ç»‡æˆæ®µè½\n",
    "    for line in full_text.split('\\n'):\n",
    "        if len(line) >= min_line_length:\n",
    "            buffer += (' ' + line) if not line.endswith('-') else line.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer.strip())\n",
    "    return paragraphs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeetCode 101: A Grinding Guide (Second Edition)\n",
      "\n",
      "ç‰ˆæœ¬ï¼šæ­£å¼ç‰ˆ 2.0cï¼Œæœ€æ–°ç‰ˆè§ GitHub changgyhub/leetcode_101\n",
      "\n",
      "ä¸€ä¸ªé¢å‘æœ‰ä¸€å®šçš„ç¼–ç¨‹åŸºç¡€ï¼Œä½†ç¼ºä¹åˆ·é¢˜ç»éªŒçš„è¯»è€…çš„æ•™ç§‘ä¹¦å’Œå·¥å…·ä¹¦ã€‚\n",
      "\n",
      "2019 å¹´åº•ï¼Œæœ¬ä¹¦ç¬¬ä¸€ç‰ˆåœ¨ GitHub ä¸Šæ­£å¼å‘å¸ƒï¼Œåå“ååˆ†çƒ­çƒˆã€‚è¿‡å»çš„äº”å¹´ï¼Œä½œè€…ç§¯ç´¯äº†å¤§\n",
      "\n",
      "é‡çš„å·¥ä½œç»éªŒï¼ŒåŒæ—¶ä¹Ÿè¶Šæ¥è¶Šè§‰å¾—è¿™æœ¬ä¹¦å­˜åœ¨å¾ˆå¤šçº°æ¼å’Œä¸å®Œå–„ä¹‹å¤„ã€‚äºæ˜¯åœ¨ 2024 å¹´åº•ï¼Œä½œè€…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs = extract_text_from_pdf(\"file/LeetCode 101 - A Grinding Guide.pdf\", min_line_length=30)\n",
    "for paragraph in paragraphs[:5]:\n",
    "    print(paragraph + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚æˆ‘ç†Ÿç»ƒæŒæ¡å¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä¸­æ–‡ã€è‹±æ–‡ã€å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ç­‰ã€‚\\n\\nå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 12, 'total_tokens': 302, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 205, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_name': 'Qwen/QwQ-32B', 'system_fingerprint': '', 'id': '01968030c05b25b7cb43503f7be9c1f6', 'finish_reason': None, 'logprobs': None}, id='run-67e9c66e-e3e6-4043-9932-d14203a02193-0', usage_metadata={'input_tokens': 12, 'output_tokens': 290, 'total_tokens': 302, 'input_token_details': {}, 'output_token_details': {'reasoning': 205}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.env_util import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    model = ChatOpenAI(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=os.getenv(\"MODEL_NAME\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return model.invoke(message)\n",
    "\n",
    "get_completion(\"ä½ æ˜¯è°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦èƒ¡ç¼–ä¹±é€ ã€‚\n",
    "\n",
    "å·²çŸ¥ä¿¡æ¯ï¼š\n",
    "{context}\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜ï¼š\n",
    "{query}\n",
    "\n",
    "å¦‚æœå·²çŸ¥ä¿¡æ¯ä¸åŒ…å«ç”¨æˆ·é—®é¢˜çš„ç­”æ¡ˆï¼Œæˆ–è€…å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦èƒ¡ç¼–ä¹±é€ ã€‚\n",
    "è¯·ä¸è¦è¾“å‡ºå·²çŸ¥ä¿¡æ¯ä¸­ä¸åŒ…å«çš„ä¿¡æ¯æˆ–ç­”æ¡ˆã€‚\n",
    "è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(prompt_template, **kwargs):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰å‚æ•°æ¸²æŸ“ Prompt æ¨¡æ¿\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = \"\\n\\n\".join(v)\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return prompt_template.format(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‘é‡é—´ç›¸ä¼¼åº¦çš„è®¡ç®—\n",
    "\n",
    "ä½™å¼¦ç›¸ä¼¼åº¦é€šè¿‡å‘é‡ç‚¹ç§¯ä¸æ¨¡é•¿ä¹˜ç§¯çš„æ¯”å€¼è®¡ç®—ï¼Œå…¬å¼å¦‚ä¸‹ï¼š \n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "L2 è·ç¦»æ˜¯å‘é‡å¯¹åº”å…ƒç´ å·®çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ï¼Œå…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    ''' ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰ '''\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def l2_distance(A, B):\n",
    "    ''' æ¬§å¼è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ '''\n",
    "    x = np.asarray(A)\n",
    "    y = np.asarray(B)\n",
    "    return norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åµŒå…¥æ¨¡å‹\n",
    "\n",
    "[åµŒå…¥æ¨¡å‹åº“](https://www.modelscope.cn/models?page=1&tasks=sentence-embedding&type=nlp)\n",
    "\n",
    "æ ‡å‡†\n",
    "\n",
    "- **æ‰¾éœ€æ±‚ç›¸å…³çš„è¯­æ–™åº“æ¥è¿›è¡Œæ–‡æœ¬å‘é‡è½¬æ¢æµ‹è¯•ï¼Œè¿›è¡Œè¯„ä¼°**\n",
    "- å¤§å¤šæ•°åœºæ™¯ä¸‹ï¼Œå¼€æºçš„åµŒå…¥æ¨¡å‹ä½¿ç”¨æ•ˆæœéƒ½ä¸€èˆ¬ï¼Œè¦è¿›è¡Œæ£€ç´¢å¬å›ç‡ï¼Œå»ºè®®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "- åµŒå…¥æ¨¡å‹çš„ç»´åº¦è¶Šé«˜ï¼Œè¡¨ç¤ºç‰¹å¾ç»†èŠ‚æå–è¶Šä¸°å¯Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dimension: 1024\n",
      "Fist 10 Dimensions: [0.05262557417154312, 0.02764252945780754, 0.07897865772247314, -0.0029994763899594545, 0.0073790643364191055, 0.026514261960983276, 0.009670855477452278, -0.0016747706104069948, 0.045372430235147476, 0.004772466607391834]\n",
      "\n",
      "[ARK] Total Dimension: 4096\n",
      "[ARK] Fist 10 Dimensions: [0.2421875, 3.859375, -0.482421875, -3.96875, 0.74609375, -2.625, 1.6015625, 3.359375, 0.3984375, 3.25]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "def get_embeddings(texts: List[str]):\n",
    "    ''' OpenAI Embeddings '''\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        base_url=os.getenv('OPENAI_BASE_URL'),\n",
    "        model=os.getenv('EMBEDDING_MODEL_NAME')\n",
    "    )\n",
    "    return embedding_model.embed_documents(texts)\n",
    "\n",
    "def get_ark_embeddings(texts: List[str]):\n",
    "    client = Ark(api_key=os.getenv('ARK_API_KEY'))\n",
    "    return client.embeddings.create(\n",
    "        model=\"doubao-embedding-large-text-240915\",\n",
    "        input=texts\n",
    "    )\n",
    "\n",
    "test_texts = ['åƒå®Œæµ·é²œå¯ä»¥å–ç‰›å¥¶å—ï¼Ÿ']\n",
    "vec = get_embeddings(test_texts)[0]\n",
    "print(f'Total Dimension: {len(vec)}')\n",
    "print(f'Fist 10 Dimensions: {vec[:10]}')\n",
    "\n",
    "vec = get_ark_embeddings(test_texts).data[0].embedding\n",
    "print(f'\\n[ARK] Total Dimension: {len(vec)}')\n",
    "print(f'[ARK] Fist 10 Dimensions: {vec[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query ä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»ä¸ºï¼š1.0\n",
      "query ä¸æ–‡æ¡£çš„ä½™å¼¦è·ç¦»ä¸ºï¼š\n",
      "0.8851309597447\n",
      "0.8601865743943213\n",
      "0.802517564858825\n",
      "0.7592321142985579\n",
      "0.796052817036916\n",
      "********************************************************************************\n",
      "query ä¸è‡ªå·±çš„L2è·ç¦»ä¸ºï¼š0.0\n",
      "query ä¸æ–‡æ¡£çš„L2è·ç¦»ä¸ºï¼š\n",
      "63.85760211053751\n",
      "69.9480541076191\n",
      "82.65289032621875\n",
      "91.29779700506289\n",
      "84.24111387780202\n"
     ]
    }
   ],
   "source": [
    "query = \"å›½é™…äº‰ç«¯\"\n",
    "# å‰ä¸¤æ¡ä¸ºå›½é™…äº‰ç«¯\n",
    "documents = [\n",
    "    \"è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œåœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š\",\n",
    "    \"åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤\",\n",
    "    \"æ—¥æœ¬æ­§é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤\",\n",
    "    \"å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥\",\n",
    "    \"æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ\"\n",
    "]\n",
    "\n",
    "# query_vector = get_embeddings([query])[0]\n",
    "# doc_vectors = get_embeddings(documents)\n",
    "\n",
    "# å®æµ‹ doubao Embedding æ¨¡å‹æ•ˆæœæ›´å¥½\n",
    "query_vector = get_ark_embeddings([query]).data[0].embedding\n",
    "doc_vectors = [doc_vector.embedding for doc_vector in get_ark_embeddings(documents).data]\n",
    "\n",
    "# ä½™å¼¦è·ç¦»è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼\n",
    "print(f'query ä¸è‡ªå·±çš„ä½™å¼¦è·ç¦»ä¸ºï¼š{cosine_similarity(query_vector, query_vector)}')\n",
    "print('query ä¸æ–‡æ¡£çš„ä½™å¼¦è·ç¦»ä¸ºï¼š')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{cosine_similarity(query_vector, doc_vector)}')\n",
    "\n",
    "print('*' * 80)\n",
    "\n",
    "# L2è·ç¦»ï¼ˆæ¬§å¼è·ç¦»ï¼‰è¶Šå°è¡¨ç¤ºè¶Šç›¸ä¼¼\n",
    "print(f'query ä¸è‡ªå·±çš„L2è·ç¦»ä¸ºï¼š{l2_distance(query_vector, query_vector)}')\n",
    "print('query ä¸æ–‡æ¡£çš„L2è·ç¦»ä¸ºï¼š')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{l2_distance(query_vector, doc_vector)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
