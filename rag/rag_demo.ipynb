{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.187057Z",
     "start_time": "2025-05-01T05:43:13.488502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n我是基于人工智能技术的语言模型助手，由智谱 AI 公司开发。我的核心功能是通过自然语言对话，为用户提供信息查询、知识解答、创意生成、学习辅助等服务。以下是我的主要特点：\\n\\n1. **知识库**：截至2023年10月，我接受了涵盖科技、历史、文化、生活等领域的海量数据训练，能提供广泛的知识支持。\\n\\n2. **交互能力**：\\n   - 支持多轮对话\\n   - 可处理中英文混合输入\\n   - 能理解上下文逻辑关系\\n\\n3. **应用场景**：\\n   - 学术研究辅助\\n   - 商业决策参考\\n   - 日常问题解答\\n   - 创意内容生成\\n\\n4. **技术特性**：\\n   - 基于Transformer架构\\n   - 采用1750亿参数的GPT-4架构\\n   - 支持上下文记忆（约4096个token）\\n\\n5. **使用限制**：\\n   - 知识截止日期为2023年10月\\n   - 无法进行实时网络搜索\\n   - 不具备物理世界交互能力\\n   - 需要用户主动提供背景信息\\n\\n6. **隐私保护**：\\n   - 不存储对话记录\\n   - 不收集个人身份信息\\n   - 符合GDPR等数据保护规范\\n\\n如果您有具体问题或需要帮助完成某项任务，可以随时告诉我，我会尽力提供专业、准确的解答。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 520, 'prompt_tokens': 10, 'total_tokens': 530, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 218, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_name': 'THUDM/GLM-Z1-32B-0414', 'system_fingerprint': '', 'id': '01968a5ed8ba659eb71e7dab5f909f26', 'finish_reason': 'stop', 'logprobs': None}, id='run-223e7b67-21ce-4493-a81d-32c6e217bd7b-0', usage_metadata={'input_tokens': 10, 'output_tokens': 520, 'total_tokens': 530, 'input_token_details': {}, 'output_token_details': {'reasoning': 218}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.env_util import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    model = ChatOpenAI(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=os.getenv(\"MODEL_NAME\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return model.invoke(message)\n",
    "\n",
    "get_completion(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.199661Z",
     "start_time": "2025-05-01T05:43:24.195325Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人，你的任务是根据下述给定的已知信息回答用户问题。如果你不知道答案，就回答不知道，不要胡编乱造。\n",
    "\n",
    "已知信息：\n",
    "{context}\n",
    "\n",
    "用户问题：\n",
    "{query}\n",
    "\n",
    "如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，就回答不知道，不要胡编乱造。\n",
    "请不要输出已知信息中不包含的信息或答案。\n",
    "请用中文回答用户问题。\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(prompt_template, **kwargs):\n",
    "    \"\"\"\n",
    "    自定义参数渲染 Prompt 模板\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = \"\\n\\n\".join(v)\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return prompt_template.format(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量间相似度的计算\n",
    "\n",
    "余弦相似度通过向量点积与模长乘积的比值计算，公式如下： \n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "L2 距离是向量对应元素差的平方和的平方根，公式如下：\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.224854Z",
     "start_time": "2025-05-01T05:43:24.222259Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    \"\"\" 余弦相似度（越大越相似） \"\"\"\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def l2_distance(A, B):\n",
    "    \"\"\" 欧式距离（越小越相似） \"\"\"\n",
    "    x = np.asarray(A)\n",
    "    y = np.asarray(B)\n",
    "    return norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入（Embedding）模型\n",
    "\n",
    "[嵌入模型库](https://www.modelscope.cn/models?page=1&tasks=sentence-embedding&type=nlp)\n",
    "\n",
    "标准\n",
    "\n",
    "- **找需求相关的语料库来进行文本向量转换测试，进行评估**\n",
    "- 向量模型的精确度直接影响相似度检索的文档召回率\n",
    "- 大多数场景下，开源的嵌入模型使用效果都一般，要进行检索召回率，建议对模型进行微调\n",
    "- 嵌入模型的维度越高，表示特征细节提取越丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.634795Z",
     "start_time": "2025-05-01T05:43:24.251424Z"
    }
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'code': 20012, 'message': 'Model does not exist. Please check it carefully.', 'data': None}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# def get_embeddings(texts, model=\"text-embedding-ada-002\", dimensions=None):\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m#     '''封装 OpenAI 的 Embedding 模型接口'''\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m#     if model == \"text-embedding-ada-002\":\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     29\u001B[39m \u001B[38;5;66;03m#         data = client.embeddings.create(input=texts, model=model).data\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m#     return [x.embedding for x in data]\u001B[39;00m\n\u001B[32m     32\u001B[39m test_texts = [\u001B[33m'\u001B[39m\u001B[33m吃完海鲜可以喝牛奶吗？\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m vec = \u001B[43mget_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_texts\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m     34\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mTotal Dimension: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(vec)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m     35\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mFist 10 Dimensions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvec[:\u001B[32m10\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mget_embeddings\u001B[39m\u001B[34m(texts)\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\" OpenAI Embeddings \"\"\"\u001B[39;00m\n\u001B[32m      7\u001B[39m embedding_model = OpenAIEmbeddings(\n\u001B[32m      8\u001B[39m     api_key=os.getenv(\u001B[33m'\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m'\u001B[39m),\n\u001B[32m      9\u001B[39m     base_url=os.getenv(\u001B[33m'\u001B[39m\u001B[33mOPENAI_BASE_URL\u001B[39m\u001B[33m'\u001B[39m),\n\u001B[32m     10\u001B[39m     model=os.getenv(\u001B[33m'\u001B[39m\u001B[33mEMBEDDING_MODEL_NAME\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     11\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membedding_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43membed_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:576\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    574\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    575\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:471\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m471\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    475\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:128\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    122\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    123\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    124\u001B[39m             ).tolist()\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/openai/_base_client.py:1247\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1233\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1234\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1235\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1242\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1243\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1244\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1245\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1246\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1247\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/openai/_base_client.py:920\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[39m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    918\u001B[39m     retries_taken = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m920\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    921\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    922\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    923\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/openai/_base_client.py:1028\u001B[39m, in \u001B[36mSyncAPIClient._request\u001B[39m\u001B[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[39m\n\u001B[32m   1025\u001B[39m         err.response.read()\n\u001B[32m   1027\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1028\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1030\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_response(\n\u001B[32m   1031\u001B[39m     cast_to=cast_to,\n\u001B[32m   1032\u001B[39m     options=options,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1036\u001B[39m     retries_taken=retries_taken,\n\u001B[32m   1037\u001B[39m )\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'code': 20012, 'message': 'Model does not exist. Please check it carefully.', 'data': None}"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "def get_embeddings(texts: List[str]):\n",
    "    \"\"\" OpenAI Embeddings \"\"\"\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        base_url=os.getenv('OPENAI_BASE_URL'),\n",
    "        model=os.getenv('EMBEDDING_MODEL_NAME')\n",
    "    )\n",
    "    return embedding_model.embed_documents(texts)\n",
    "\n",
    "def get_ark_embeddings(texts: List[str]):\n",
    "    client = Ark(api_key=os.getenv('ARK_API_KEY'))\n",
    "    return client.embeddings.create(\n",
    "        model=\"doubao-embedding-large-text-240915\",\n",
    "        input=texts\n",
    "    )\n",
    "\n",
    "# def get_embeddings(texts, model=\"text-embedding-ada-002\", dimensions=None):\n",
    "#     '''封装 OpenAI 的 Embedding 模型接口'''\n",
    "#     if model == \"text-embedding-ada-002\":\n",
    "#         dimensions = None\n",
    "#     if dimensions:\n",
    "#         data = client.embeddings.create(\n",
    "#             input=texts, model=model, dimensions=dimensions).data\n",
    "#     else:\n",
    "#         data = client.embeddings.create(input=texts, model=model).data\n",
    "#     return [x.embedding for x in data]\n",
    "\n",
    "test_texts = ['吃完海鲜可以喝牛奶吗？']\n",
    "vec = get_embeddings(test_texts)[0]\n",
    "print(f'Total Dimension: {len(vec)}')\n",
    "print(f'Fist 10 Dimensions: {vec[:10]}')\n",
    "\n",
    "vec = get_ark_embeddings(test_texts).data[0].embedding\n",
    "print(f'\\n[ARK] Total Dimension: {len(vec)}')\n",
    "print(f'[ARK] Fist 10 Dimensions: {vec[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.636822Z",
     "start_time": "2025-05-01T05:38:53.645326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 与自己的余弦相似度为：1.0\n",
      "query 与文档的余弦相似度为：\n",
      "0.8854540946810185\n",
      "0.8605012890827118\n",
      "0.80269178686373\n",
      "0.7593635176801089\n",
      "0.7953524476835953\n",
      "********************************************************************************\n",
      "query 与自己的L2距离为：0.0\n",
      "query 与文档的L2距离为：\n",
      "63.82647284306631\n",
      "70.09578368205798\n",
      "82.47880162653664\n",
      "91.39691180339793\n",
      "83.86019628342954\n"
     ]
    }
   ],
   "source": [
    "query = \"国际争端\"\n",
    "# 前两条为国际争端\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本歧阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\"\n",
    "]\n",
    "\n",
    "# query_vector = get_embeddings([query])[0]\n",
    "# doc_vectors = get_embeddings(documents)\n",
    "\n",
    "# 实测 doubao Embedding 模型效果更好\n",
    "query_vector = get_ark_embeddings([query]).data[0].embedding\n",
    "doc_vectors = [doc_vector.embedding for doc_vector in get_ark_embeddings(documents).data]\n",
    "\n",
    "# 余弦距离越大表示越相似\n",
    "print(f'query 与自己的余弦相似度为：{cosine_similarity(query_vector, query_vector)}')\n",
    "print('query 与文档的余弦相似度为：')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{cosine_similarity(query_vector, doc_vector)}')\n",
    "\n",
    "print('*' * 80)\n",
    "\n",
    "# L2距离（欧式距离）越小表示越相似\n",
    "print(f'query 与自己的L2距离为：{l2_distance(query_vector, query_vector)}')\n",
    "print('query 与文档的L2距离为：')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{l2_distance(query_vector, doc_vector)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量数据库 Chroma\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "官方文档：https://docs.trychroma.com/docs/overview/introduction<br>\n",
    "<b>扩展阅读：</b><a herf=\"https://guangzhengli.com/blog/zh/vector-database\">https://guangzhengli.com/blog/zh/vector-database</a>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>澄清几个关键概念：</b><ul>\n",
    "    <li>向量数据库的意义是快速的检索；</li>\n",
    "    <li>向量数据库本身不生成向量，向量是由 Embedding 模型产生的；</li>\n",
    "    <li>向量数据库与传统的关系型数据库是互补的，不是替代关系，在实际应用中根据实际需求经常同时使用。</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.637504Z",
     "start_time": "2025-05-01T05:38:54.581019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "医学的发展不能脱离它所处的时代。医学思想和实践来自与之相适应的知识环境，同时又为拓展和丰富人类的知识\n",
      "\n",
      "贡献力量。从原始社会到现在几千年的发展历程，医学的发展道路艰难曲折，不仅囊括了医学的各门学科，而且还\n",
      "\n",
      "涉及丰富多彩的人类医疗卫生活动。医学的发展凝聚着一代又一代先行者的心血和智慧，它既是人类对自身疾病与\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    \"\"\"Extract text from a PDF file.\n",
    "    Args:\n",
    "        filename (str): Path to the PDF file.\n",
    "        page_numbers (list[int], optional): List of page numbers to extract. If None, extract all pages.\n",
    "        min_line_length (int, optional): Minimum length of a line to be included in the output.\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    buffer, full_text = '', ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    for line in full_text.split('\\n'):\n",
    "        if len(line) >= min_line_length:\n",
    "            buffer += (' ' + line) if not line.endswith('-') else line.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer.strip())\n",
    "    return paragraphs  \n",
    "\n",
    "# pdfminer 的效果不太好，这里只演示效果，实际用 markdown 文件\n",
    "paragraphs = extract_text_from_pdf(\"file/医学史.pdf\", min_line_length=45)\n",
    "for paragraph in paragraphs[:3]:\n",
    "    print(paragraph + '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.638965Z",
     "start_time": "2025-05-01T05:38:54.739814Z"
    }
   },
   "outputs": [],
   "source": [
    "# 为了演示方便，我们只取两页（第一章/Page 3、Page 4）\n",
    "# paragraphs = extract_text_from_pdf(\n",
    "#     \"file/医学史.pdf\",\n",
    "#     page_numbers=[3, 3],\n",
    "#     min_line_length=10\n",
    "# )\n",
    "\n",
    "# for paragraph in paragraphs:\n",
    "#     print(paragraph + '\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割/检索 markdown 文件\n",
    "\n",
    "**文本分割的粒度**  \n",
    "1. 粒度太大可能导致检索不精准，粒度太小可能导致信息不全面\n",
    "2. 问题的答案可能跨越两个片段\n",
    "\n",
    "LangChain 文本分割的策略：\n",
    "1. **基于字符的分割（`CharacterTextSplitter`）**\n",
    "\n",
    "    按照指定的字符进行分割，默认使用换行符和空格作为分隔符。通过设置 `chunk_size` 和 `chunk_overlap` 参数，可以控制每个文本块的大小和重叠部分。\n",
    "\n",
    "    ```python\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "    text = \"这是一段示例文本，用于演示文本分割的策略。\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=10,\n",
    "        chunk_overlap=2\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(chunks)\n",
    "    ```\n",
    "\n",
    "2. **基于 token 的分割（`TokenTextSplitter`）**\n",
    "\n",
    "    `TokenTextSplitter` 基于 `token` 进行分割，而不是字符。标记是自然语言处理中对文本进行分词后的基本单元。通过设置 `chunk_size 和 `chunk_overlap` 参数，可以控制每个文本块包含的标记数量。\n",
    "\n",
    "\n",
    "3. **按文档结构分割（`MarkdownTextSplitter`、`HtmlTextSplitter`）**\n",
    "\n",
    "    `MarkdownTextSplitter` 专门用于分割 Markdown 文档，它会根据 Markdown 的语法结构（如标题、列表等）进行分割，确保分割后的文本块具有一定的语义完整性。  \n",
    "    `HtmlTextSplitter` 用于分割 HTML 文档，它会根据 HTML 的标签结构进行分割，去除 HTML 标签，只保留文本内容，并将文本分割成合适的块。\n",
    "\n",
    "4. **递归分割（`RecursiveCharacterTextSplitter`）**\n",
    "\n",
    "    它会尝试按照多个分隔符进行分割，优先使用较长的分隔符，以确保分割后的文本块具有较好的语义完整性。如果使用较长的分隔符无法满足 chunk_size 的要求，则会尝试使用较短的分隔符。\n",
    "\n",
    "    ```python\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text = \"这是一段示例文本，用于演示递归文本分割的策略。\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"。\", \"，\", \" \"],\n",
    "        chunk_size=10,\n",
    "        chunk_overlap=2\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(chunks)\n",
    "    ```\n",
    "\n",
    "5. **基于语义分割**\n",
    "\n",
    "    1. 根据句子、段落或主题等有语义的单元进行分割；\n",
    "    2. 为每个片段创建 Embeddig；\n",
    "    3. 对相邻的片段计算余弦相似度，选择相似度最高的两个片段进行合并，直到余弦相似度显著下降位置。\n",
    "\n",
    "    > 或者使用早期预训练模型（如 BERT）中用于增强句子级语义理解的 NSP 任务（Next Sentence Prediction，下一句预测任务） 方法。\n",
    "\n",
    "6. **基于 LLM 分割**\n",
    "\n",
    "    通过 Prompt + LLM 的方式，将文本分割成多个片段，每个片段包含一个语义单元。这种方法将保证较高的语义准确性，因为 LLM 可以更好地理解文本的语义。\n",
    "\n",
    "---\n",
    "\n",
    "在实际应用中，选择合适的文本分割器应根据具体需求进行。例如：\n",
    "- 对于简单文本，可以选择 `CharacterTextSplitter`。\n",
    "- 处理长文本或需要上下文信息的场合，推荐使用` RecursiveCharacterTextSplitter` 或 `TokenTextSplitter`。\n",
    "- 中文文章推荐用 `RecursiveCharacterTextSplitter`\n",
    "- 处理 `Markdown` 文档时，`MarkdownTextSplitter` 是最佳选择，而对于`LaTeX`文档，则应使用`LatexTextSplitter`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T05:43:24.639355Z",
     "start_time": "2025-05-01T05:38:54.750633Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# chunk_size 一般根据文档内容或大小来设置\n",
    "# overlap_size 一般设置 chunk_size 大小的10%-20%之间\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''按指定 chunk_size 和 overlap_size 交叠割文本'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # 向前计算重叠部分\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # 向后计算当前chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "此处 sent_tokenize 为针对英文的实现，针对中文的实现请参考 <a herf='./chinese_utils.py'>chinese_utils.py</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:04.041116Z",
     "start_time": "2025-05-01T06:22:04.035093Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def read_markdown_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：未找到文件 {file_path}。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"错误：读取文件时发生未知错误 {e}。\")\n",
    "        return None\n",
    "\n",
    "file_path = os.path.abspath(os.path.join(os.getcwd(), 'file/医学史.md'))\n",
    "markdown_content = read_markdown_file(file_path)\n",
    "\n",
    "\n",
    "\n",
    "docs = []\n",
    "if markdown_content:\n",
    "    # 创建 Markdown 文本分割器\n",
    "    # markdown_splitter = MarkdownTextSplitter(chunk_size=512, chunk_overlap=250)\n",
    "    markdown_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=256)\n",
    "\n",
    "    # 分割 Markdown 内容为多个 Document\n",
    "    docs = markdown_splitter.create_documents([markdown_content])\n",
    "\n",
    "    # 打印每个 Document 的内容\n",
    "    for i, doc in enumerate(docs[:2]):\n",
    "        print(f\"************************** Document {i + 1}: *********************************\")\n",
    "        print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** Document 1: *********************************\n",
      "医学的发展不能脱离它所处的时代。医学思想和实践来自与之相适应的知识环境，同时又为拓展和丰富人类的知识贡献力量。从原始社会到现在几千年的发展历程，医学的发展道路艰难曲折，不仅囊括了医学的各门学科，而且还涉及丰富多彩的人类医疗卫生活动。医学的发展凝聚着一代又一代先行者的心血和智慧，它既是人类对自身疾病与健康及其关系的认识史，也是一部伴随着社会生产发展，由经验到科学，由低级到高级、由单一到综合逐渐进化的发展史，既是科学技术进步的一个缩形，也是人类文化史的一个重要组成。\n",
      "\n",
      "## 一、古代医学\n",
      "受古代生产力及科学技术水平的限制，古代医学知识多来源于医疗实践经验的积累，夹杂着唯心主义和迷信思想。宗教和文学在一定程度上促进了古代医学的发展。在数千年的历史长河中，古代医学经历了漫长的发展历程。\n",
      "\n",
      "### （一）史前医学\n",
      "在漫长的 300 至万年前的原始社会中，原始人类在同疾病斗争的过程中逐步认识了各种植物、动物和矿物等的药效，开辟了以经验为起源的各项医药活动。在原始社会末期，已有了断肢术、阉割术，穿颅术、剖宫产术等外科手术和相应的外伤治疗外用药物。这些进步，也与社会生产工具的发明和改进密切相关。\n",
      "************************** Document 2: *********************************\n",
      "### （一）史前医学\n",
      "在漫长的 300 至万年前的原始社会中，原始人类在同疾病斗争的过程中逐步认识了各种植物、动物和矿物等的药效，开辟了以经验为起源的各项医药活动。在原始社会末期，已有了断肢术、阉割术，穿颅术、剖宫产术等外科手术和相应的外伤治疗外用药物。这些进步，也与社会生产工具的发明和改进密切相关。\n",
      "\n",
      "由于原始社会的生产力低下，人们不能正确理解疾病和死亡，认为疾病是由一种超自然的力量所形成和主宰的，认为自然界的一切现象都由一种超自然的实体控制，从而产生了万物有灵的观念：山神、河怪、精灵、疯魔等都可导致人类疾病。在这些认识支配下，产生了招魂、驱鬼、敬神、祷告等病方法，产生了祭司和巫师，也产生了图腾崇拜，医学被打上了原始宗教的烙印。\n",
      "\n",
      "### （二）古代东方医学\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:07.305099Z",
     "start_time": "2025-05-01T06:22:07.300702Z"
    }
   },
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn, batch_size=None):\n",
    "        # 内存模式\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "        # 数据持久化\n",
    "        # chroma_client = chromadb.PersistentClient(path=\"./chroma\")\n",
    "\n",
    "        # NOTE: 清空数据，为了演示，实际不需要每次 reset()，并且是不可逆的\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(\n",
    "            name=collection_name, \n",
    "            metadata={\"hnsw:space\": \"cosine\"} # l2 is the default\n",
    "        )\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents, batch_size=None):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        n = len(documents)\n",
    "        if batch_size is None:\n",
    "            self.collection.add(\n",
    "                embeddings=self.embedding_fn(documents),        # 每个文档的向量\n",
    "                documents=documents,                            # 文档的原文\n",
    "                ids=[f\"id{i}\" for i in range(n)]   # 每个文档的 id\n",
    "            )\n",
    "        else:\n",
    "            for i in range(0, n, batch_size):\n",
    "                end_idx = min(i + batch_size, n)\n",
    "                self.collection.add(\n",
    "                    embeddings=self.embedding_fn(documents[i:end_idx]),        # 每个文档的向量\n",
    "                    documents=documents[i:end_idx],                            # 文档的原文\n",
    "                    ids=[f\"id{i}\" for i in range(i, end_idx)]   # 每个文档的 id\n",
    "                )\n",
    "        print(f\"\\n 😁 添加{n}份文档成功 😁\\n\")\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** 这里如果提示 `sqlite` 版本过低，可以安装 `pysqlite3-binary`，然后在虚拟环境的 `chromadb/__init__.py` 中添加如下代码：\n",
    "```python\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Chroma 里，`collection.query` 返回结果中的 `distances` 表示查询向量和集合中返回的文档向量之间的距离。一般而言，这个值越小代表两个向量越相似。\n",
    "1. 余弦距离（`distance_metric='cosine'`）\n",
    "含义：余弦距离基于余弦相似度得出，公式为 `余弦距离 = 1 - 余弦相似度`。余弦相似度衡量的是两个向量夹角的余弦值，范围是 [-1, 1]。当两个向量方向完全相同时，余弦相似度为 1，余弦距离为 0；当两个向量方向完全相反时，余弦相似度为 -1，余弦距离为 2。\n",
    "评判标准：余弦距离越接近 0，意味着查询向量和文档向量的方向越相近，也就是两个向量越相似。所以在使用余弦距离时，`distances` 的值越小越好。\n",
    "\n",
    "2. 欧几里得距离（`distance_metric='l2'`）\n",
    "含义：欧几里得距离指的是 n 维空间中两个点之间的直线距离。\n",
    "评判标准：欧几里得距离为 0 时，表示两个向量完全相同；值越大，表明两个向量在空间中的距离越远，相似度越低。所以使用欧几里得距离时，同样是 `distances` 的值越小越好。\n",
    "\n",
    "3. 内积距离（`distance_metric='ip'`）\n",
    "含义：内积距离基于向量的内积来计算。内积反映了两个向量在方向上的一致性和长度的乘积。\n",
    "评判标准：一般来说，内积值越大，两个向量越相似，但在 Chroma 中使用内积作为距离度量时，通常会对其进行转换，使得距离值越小表示越相似。\n",
    "\n",
    "在 Chroma 中，`collection.query` 返回的结果默认是按照 距离从小到大排序的，因此 `distances` 越小的结果会排在越前面。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:13.834917Z",
     "start_time": "2025-05-01T06:22:10.847114Z"
    }
   },
   "source": [
    "# 提取文档的文本内容\n",
    "document_texts = [doc.page_content for doc in docs]\n",
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(document_texts)\n",
    "\n",
    "user_query = \"\"\"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道\"\"\"\n",
    "results = vector_db.search(user_query, 5)\n",
    "\n",
    "for idx, para in enumerate(results['documents'][0]):\n",
    "    print(f\"**************** 😁 rank:{idx +1} | distance: {results['distances'][0][idx]} *****************\")\n",
    "    print(para+\"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 😁 添加60份文档成功 😁\n",
      "\n",
      "**************** 😁 rank:1 | distance: 0.33148616552352905 *****************\n",
      "1953年，美国分子生物学家洪森 (James Dewey Watson.1928-)和英国物理学家克里克 (FrancisHarry Compton Crick,1916--2004）以及英国物理学家威尔金斯 (Maurice Hugh Frederick Wilkins, 1916--2004) 发现并阐明了 DNA 分子的双螺旋结构，奠定了分子生物学的基础。他们三人分获 1962年诺贝尔生理学或医学奖。1965年，我国科学家在世界上首次用化学方法合成了牛胰岛素；之后，美国科学家也合成了含有206个核苷酸的 DNA 大分子。70年代发现了逆转录酶和限制性内切酶，促进了基因工程的发展。80年代初，临床开始应用基因工程治疗疾病，如用单克隆技术治疗癌症。人们在发酵工业中大量生产胰岛素，保证了临床用药的需要，降低了成本，减轻了病人的经济负担。\n",
      "\n",
      "（2）医学遗传学的发展：\n",
      "\n",
      "医学遗传学是研究人类疾病与遗传的关系，研究人类遗传病形成的机制和遗传方式，以及遗传病的诊断、治疗、预后、复发危险和预防的科学，是医学与遗传学相结合的边缘学科。\n",
      "\n",
      "**************** 😁 rank:2 | distance: 0.33584147691726685 *****************\n",
      "显微镜的应用为 19 世纪细胞学的建立打下了良好的基础。\n",
      "\n",
      "#### 3．医学三学派的成熟\n",
      "\n",
      "在17世纪，由于物理学、化学和生物学的进步，使一些学者主张以单一学科的理论来解释生命现象和病理现象，出现三个学派。\n",
      "\n",
      "（1）物理学派：代表人物是法国数学家、物理学家笛卡尔。他认为：“宇宙是一个庞大的机械，人的身体也是一部精细的机械，从宏观到微观，所有物体无一不可用机械原理来阐明。”身体的一切疼痛、恐怖表现都是机械的反应。伽利略的学生波累利认为肌肉运动是一种力学原理，人心脏的搏动、胃肠 运动都符合力学原理。他甚至认为胃的消化功能就是摩擦力作用的结果。\n",
      "\n",
      "**************** 😁 rank:3 | distance: 0.3737780451774597 *****************\n",
      "17世纪的医学进步得益于伽利略（Galileo Galilei. 1564-1642)和刻卜勒（Jobannes Kepler,1571-1630）等一批杰出科学家的成就。例如帕多瓦大学的教授桑克托留斯（Sanctorius,1561-1636) 所设计的最早的体温计和脉搏计是根据伽利略的发明而加以改制的。\n",
      "\n",
      "#### 1．生理学的进展\n",
      "\n",
      "17世纪初，由于量度的应用，使生命科学的研究步入科学轨道。其标志之一是英国医学家威康•哈维（ Wiliam Harvey,1578-—1657） 发现血液循环，创建了血流循环学说。从而使生理学从解剖学中分立出来。哈维首先应用活体解剖的实验方法，并应用度量的概念，精确地计算出心脏每分钟搏出血量和每小时搏出血量。他于1628年发表了著作心血运动论）(The Movement of the heart and the Blood), 标志着血液循环理论的建立。恩格斯对哈维的发现做出这样的评价：“由于哈维发现血液循环，而把生理学确立为一门科学。”生理学家巴甫洛夫 (Ivan Pavlov,1849—1936）也评价说；“哈维的研究为动物生理学奠定了基础。”\n",
      "\n",
      "**************** 😁 rank:4 | distance: 0.42035651206970215 *****************\n",
      "意大利人马尔匹基用显微镜观察了动、植物的微细构造，开拓了组织学分野。18世纪末，研究个体发生的胚胎学开始起步。德国胚胎学家冯贝尔（Karl Emst Von Bear.1792-1876）提出“胚层说”，认为除极低等的动物外，一切动物最终由胚层发育成动物器官。《动物的发育》是他出版的胚胎学专著。\n",
      "\n",
      "19世纪意大利学者高尔基（Camello Golgi, 1843-1926)首创镀银浸染神经元技术，被称为神经解剖学创始人之一。\n",
      "\n",
      "#### 3．药理学\n",
      "\n",
      "19世纪化学技术的进步使提取药用的植物有效成分成为可能，例如 1806 年从鸦片中提取出吗啡；1817 年从吐根中提取出叶根碱；1818 年从马钱子中提取士的宁；1821 年从咖啡中提取出咖啡因等；1826年从金鸡纳树皮提取出奎宁。19世纪初，在德国建立了第一个药理实验室，出版了第一本药理教科书，标志着独立的药理学科的建立。19世纪中叶，已能人工合成一次药物，如人工合成尿素、氯仿、苯胺、硫酸盐类解热镇痛剂等。人们以临床医学和生理学为基础，以动物实验为手段，开始探讨药物的作用及其机制，从而建立了实验药理学。\n",
      "\n",
      "#### 4．病理生理学\n",
      "\n",
      "**************** 😁 rank:5 | distance: 0.6202527284622192 *****************\n",
      "我国现代临床医学在基础医学、预防医学、药学的协同支撑下，在物理诊断学、实验诊断学、传染病学与寄生虫病学、内科学、地方病学、外科学、妇产科学、儿科学、眼科学、耳鼻咽喉科学、皮肤性病学、口腔医学、精神病学、神经病学、营养与食品卫生学、放射医学、护理学、临床肿瘤学、核医学等方面，均取得了显著的发展，学科体系齐全，技术装备先进，技术水平提高，国民健康得到有力保障。\n",
      "\n",
      "### （二）21世纪医学的进展趋势\n",
      "\n",
      "21 世纪的医学将进入高科技时代，医学的理论和技术将有更大更深入的发展，从根本上解除最严重疾病对人类的威胁。分子生物学、系统生物学与生物医学、预防医学、转化医学、个体化医学、精准医学、医学整合等领域将是21世纪医学发展的优先领域。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于向量检索的RAG"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:19.040113Z",
     "start_time": "2025-05-01T06:22:19.036570Z"
    }
   },
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=3):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, context=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:46.682443Z",
     "start_time": "2025-05-01T06:22:19.807057Z"
    }
   },
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(vector_db,llm_api=get_completion)\n",
    "\n",
    "user_query = \"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道?\"\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "量度的应用。\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索后排序（rerank）\n",
    "\n",
    "从向量数据库里召回后，有时候相关性高的文档不一定排在前面，需要根据相关性排序。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:22:51.761569Z",
     "start_time": "2025-05-01T06:22:51.296158Z"
    }
   },
   "source": [
    "user_query = \"\"\"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道\"\"\"\n",
    "results = vector_db.search(user_query, 5)\n",
    "\n",
    "# 这里可以看出，最相关的文档为第3个文档\n",
    "for idx, para in enumerate(results['documents'][0]):\n",
    "    print(f\"**************** rank:{idx +1} | distance: {results['distances'][0][idx]} *****************\")\n",
    "    print(para+\"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** rank:1 | distance: 0.33171558380126953 *****************\n",
      "1953年，美国分子生物学家洪森 (James Dewey Watson.1928-)和英国物理学家克里克 (FrancisHarry Compton Crick,1916--2004）以及英国物理学家威尔金斯 (Maurice Hugh Frederick Wilkins, 1916--2004) 发现并阐明了 DNA 分子的双螺旋结构，奠定了分子生物学的基础。他们三人分获 1962年诺贝尔生理学或医学奖。1965年，我国科学家在世界上首次用化学方法合成了牛胰岛素；之后，美国科学家也合成了含有206个核苷酸的 DNA 大分子。70年代发现了逆转录酶和限制性内切酶，促进了基因工程的发展。80年代初，临床开始应用基因工程治疗疾病，如用单克隆技术治疗癌症。人们在发酵工业中大量生产胰岛素，保证了临床用药的需要，降低了成本，减轻了病人的经济负担。\n",
      "\n",
      "（2）医学遗传学的发展：\n",
      "\n",
      "医学遗传学是研究人类疾病与遗传的关系，研究人类遗传病形成的机制和遗传方式，以及遗传病的诊断、治疗、预后、复发危险和预防的科学，是医学与遗传学相结合的边缘学科。\n",
      "\n",
      "**************** rank:2 | distance: 0.335904598236084 *****************\n",
      "显微镜的应用为 19 世纪细胞学的建立打下了良好的基础。\n",
      "\n",
      "#### 3．医学三学派的成熟\n",
      "\n",
      "在17世纪，由于物理学、化学和生物学的进步，使一些学者主张以单一学科的理论来解释生命现象和病理现象，出现三个学派。\n",
      "\n",
      "（1）物理学派：代表人物是法国数学家、物理学家笛卡尔。他认为：“宇宙是一个庞大的机械，人的身体也是一部精细的机械，从宏观到微观，所有物体无一不可用机械原理来阐明。”身体的一切疼痛、恐怖表现都是机械的反应。伽利略的学生波累利认为肌肉运动是一种力学原理，人心脏的搏动、胃肠 运动都符合力学原理。他甚至认为胃的消化功能就是摩擦力作用的结果。\n",
      "\n",
      "**************** rank:3 | distance: 0.37386780977249146 *****************\n",
      "17世纪的医学进步得益于伽利略（Galileo Galilei. 1564-1642)和刻卜勒（Jobannes Kepler,1571-1630）等一批杰出科学家的成就。例如帕多瓦大学的教授桑克托留斯（Sanctorius,1561-1636) 所设计的最早的体温计和脉搏计是根据伽利略的发明而加以改制的。\n",
      "\n",
      "#### 1．生理学的进展\n",
      "\n",
      "17世纪初，由于量度的应用，使生命科学的研究步入科学轨道。其标志之一是英国医学家威康•哈维（ Wiliam Harvey,1578-—1657） 发现血液循环，创建了血流循环学说。从而使生理学从解剖学中分立出来。哈维首先应用活体解剖的实验方法，并应用度量的概念，精确地计算出心脏每分钟搏出血量和每小时搏出血量。他于1628年发表了著作心血运动论）(The Movement of the heart and the Blood), 标志着血液循环理论的建立。恩格斯对哈维的发现做出这样的评价：“由于哈维发现血液循环，而把生理学确立为一门科学。”生理学家巴甫洛夫 (Ivan Pavlov,1849—1936）也评价说；“哈维的研究为动物生理学奠定了基础。”\n",
      "\n",
      "**************** rank:4 | distance: 0.42046046257019043 *****************\n",
      "意大利人马尔匹基用显微镜观察了动、植物的微细构造，开拓了组织学分野。18世纪末，研究个体发生的胚胎学开始起步。德国胚胎学家冯贝尔（Karl Emst Von Bear.1792-1876）提出“胚层说”，认为除极低等的动物外，一切动物最终由胚层发育成动物器官。《动物的发育》是他出版的胚胎学专著。\n",
      "\n",
      "19世纪意大利学者高尔基（Camello Golgi, 1843-1926)首创镀银浸染神经元技术，被称为神经解剖学创始人之一。\n",
      "\n",
      "#### 3．药理学\n",
      "\n",
      "19世纪化学技术的进步使提取药用的植物有效成分成为可能，例如 1806 年从鸦片中提取出吗啡；1817 年从吐根中提取出叶根碱；1818 年从马钱子中提取士的宁；1821 年从咖啡中提取出咖啡因等；1826年从金鸡纳树皮提取出奎宁。19世纪初，在德国建立了第一个药理实验室，出版了第一本药理教科书，标志着独立的药理学科的建立。19世纪中叶，已能人工合成一次药物，如人工合成尿素、氯仿、苯胺、硫酸盐类解热镇痛剂等。人们以临床医学和生理学为基础，以动物实验为手段，开始探讨药物的作用及其机制，从而建立了实验药理学。\n",
      "\n",
      "#### 4．病理生理学\n",
      "\n",
      "**************** rank:5 | distance: 0.6205079555511475 *****************\n",
      "我国现代临床医学在基础医学、预防医学、药学的协同支撑下，在物理诊断学、实验诊断学、传染病学与寄生虫病学、内科学、地方病学、外科学、妇产科学、儿科学、眼科学、耳鼻咽喉科学、皮肤性病学、口腔医学、精神病学、神经病学、营养与食品卫生学、放射医学、护理学、临床肿瘤学、核医学等方面，均取得了显著的发展，学科体系齐全，技术装备先进，技术水平提高，国民健康得到有力保障。\n",
      "\n",
      "### （二）21世纪医学的进展趋势\n",
      "\n",
      "21 世纪的医学将进入高科技时代，医学的理论和技术将有更大更深入的发展，从根本上解除最严重疾病对人类的威胁。分子生物学、系统生物学与生物医学、预防医学、转化医学、个体化医学、精准医学、医学整合等领域将是21世纪医学发展的优先领域。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:24:54.566242Z",
     "start_time": "2025-05-01T06:24:54.561125Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "class Rerank:\n",
    "    def __init__(self, model: str, base_url: str, api_key: str):\n",
    "        self.rerank_model = model\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        print(f'rerank 初始化成功：{self.rerank_model}, {self.base_url}, {self.api_key}')\n",
    "\n",
    "    def rerank_cloud(self, results: List[str], query: str, k=10) -> List[str]:\n",
    "        \"\"\"\n",
    "        使用云端重排序模型模型对检索结果进行重排序\n",
    "\n",
    "        :param results: 原始检索结果\n",
    "        :param query: 查询\n",
    "        :param k: 返回的top-k结果数\n",
    "        :return: 重排序后的结果\n",
    "        \"\"\"\n",
    "        # texts = [item.page_content for item in results]\n",
    "\n",
    "        url = self.base_url\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.rerank_model,\n",
    "            \"query\": query,\n",
    "            \"documents\": results,\n",
    "            \"top_n\": k,\n",
    "            \"return_documents\": False,\n",
    "            \"max_chunks_per_doc\": 512,\n",
    "            \"overlap_tokens\": 256\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "        try:\n",
    "            if response.status_code == 200:\n",
    "                response = response.json()\n",
    "                indices = [item['index'] for item in response['results']]\n",
    "                return [results[i] for i in indices]\n",
    "            else:\n",
    "                print(f\"❌ {response.json()}\")\n",
    "        except:\n",
    "            print(f'Error:network error status_code={response.status_code}')\n"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T06:30:17.949618Z",
     "start_time": "2025-05-01T06:30:17.619710Z"
    }
   },
   "source": [
    "load_dotenv()\n",
    "rerank = Rerank(\n",
    "    model=os.getenv('RERANK_MODEL_NAME'),\n",
    "    base_url=\"https://api.siliconflow.cn/v1/rerank\",\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "rerank_results = rerank.rerank_cloud(results['documents'][0], user_query, k=3)\n",
    "# 这里可以看出，rerank后，最相关的文档为第1个文档\n",
    "for idx, para in enumerate(rerank_results):\n",
    "    print(f\"**************** rank:{idx +1} *****************\")\n",
    "    print(para+\"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rerank 初始化成功：BAAI/bge-reranker-v2-m3, https://api.siliconflow.cn/v1/rerank, sk-hybehttizlquaobtbilikijqmuuyzxizjhkfqqlpkkvcvojw\n",
      "rerank cloud success\n",
      "**************** rank:1 *****************\n",
      "17世纪的医学进步得益于伽利略（Galileo Galilei. 1564-1642)和刻卜勒（Jobannes Kepler,1571-1630）等一批杰出科学家的成就。例如帕多瓦大学的教授桑克托留斯（Sanctorius,1561-1636) 所设计的最早的体温计和脉搏计是根据伽利略的发明而加以改制的。\n",
      "\n",
      "#### 1．生理学的进展\n",
      "\n",
      "17世纪初，由于量度的应用，使生命科学的研究步入科学轨道。其标志之一是英国医学家威康•哈维（ Wiliam Harvey,1578-—1657） 发现血液循环，创建了血流循环学说。从而使生理学从解剖学中分立出来。哈维首先应用活体解剖的实验方法，并应用度量的概念，精确地计算出心脏每分钟搏出血量和每小时搏出血量。他于1628年发表了著作心血运动论）(The Movement of the heart and the Blood), 标志着血液循环理论的建立。恩格斯对哈维的发现做出这样的评价：“由于哈维发现血液循环，而把生理学确立为一门科学。”生理学家巴甫洛夫 (Ivan Pavlov,1849—1936）也评价说；“哈维的研究为动物生理学奠定了基础。”\n",
      "\n",
      "**************** rank:2 *****************\n",
      "显微镜的应用为 19 世纪细胞学的建立打下了良好的基础。\n",
      "\n",
      "#### 3．医学三学派的成熟\n",
      "\n",
      "在17世纪，由于物理学、化学和生物学的进步，使一些学者主张以单一学科的理论来解释生命现象和病理现象，出现三个学派。\n",
      "\n",
      "（1）物理学派：代表人物是法国数学家、物理学家笛卡尔。他认为：“宇宙是一个庞大的机械，人的身体也是一部精细的机械，从宏观到微观，所有物体无一不可用机械原理来阐明。”身体的一切疼痛、恐怖表现都是机械的反应。伽利略的学生波累利认为肌肉运动是一种力学原理，人心脏的搏动、胃肠 运动都符合力学原理。他甚至认为胃的消化功能就是摩擦力作用的结果。\n",
      "\n",
      "**************** rank:3 *****************\n",
      "意大利人马尔匹基用显微镜观察了动、植物的微细构造，开拓了组织学分野。18世纪末，研究个体发生的胚胎学开始起步。德国胚胎学家冯贝尔（Karl Emst Von Bear.1792-1876）提出“胚层说”，认为除极低等的动物外，一切动物最终由胚层发育成动物器官。《动物的发育》是他出版的胚胎学专著。\n",
      "\n",
      "19世纪意大利学者高尔基（Camello Golgi, 1843-1926)首创镀银浸染神经元技术，被称为神经解剖学创始人之一。\n",
      "\n",
      "#### 3．药理学\n",
      "\n",
      "19世纪化学技术的进步使提取药用的植物有效成分成为可能，例如 1806 年从鸦片中提取出吗啡；1817 年从吐根中提取出叶根碱；1818 年从马钱子中提取士的宁；1821 年从咖啡中提取出咖啡因等；1826年从金鸡纳树皮提取出奎宁。19世纪初，在德国建立了第一个药理实验室，出版了第一本药理教科书，标志着独立的药理学科的建立。19世纪中叶，已能人工合成一次药物，如人工合成尿素、氯仿、苯胺、硫酸盐类解热镇痛剂等。人们以临床医学和生理学为基础，以动物实验为手段，开始探讨药物的作用及其机制，从而建立了实验药理学。\n",
      "\n",
      "#### 4．病理生理学\n",
      "\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
