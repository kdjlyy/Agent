{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 7, 'total_tokens': 78, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 35, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': None}, 'model_name': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'system_fingerprint': '', 'id': '019685943cfd8304a881b9f7a8c27a03', 'finish_reason': None, 'logprobs': None}, id='run-61d1f415-87f6-4d10-b7aa-81782c65d279-0', usage_metadata={'input_tokens': 7, 'output_tokens': 71, 'total_tokens': 78, 'input_token_details': {}, 'output_token_details': {'reasoning': 35}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.env_util import *\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_completion(prompt):\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    model = ChatOpenAI(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=os.getenv(\"MODEL_NAME\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return model.invoke(message)\n",
    "\n",
    "get_completion(\"你是谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人，你的任务是根据下述给定的已知信息回答用户问题。如果你不知道答案，就回答不知道，不要胡编乱造。\n",
    "\n",
    "已知信息：\n",
    "{context}\n",
    "\n",
    "用户问题：\n",
    "{query}\n",
    "\n",
    "如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，就回答不知道，不要胡编乱造。\n",
    "请不要输出已知信息中不包含的信息或答案。\n",
    "请用中文回答用户问题。\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(prompt_template, **kwargs):\n",
    "    \"\"\"\n",
    "    自定义参数渲染 Prompt 模板\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            val = \"\\n\\n\".join(v)\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return prompt_template.format(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量间相似度的计算\n",
    "\n",
    "余弦相似度通过向量点积与模长乘积的比值计算，公式如下： \n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "L2 距离是向量对应元素差的平方和的平方根，公式如下：\n",
    "\n",
    "$$\n",
    "d(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    ''' 余弦相似度（越大越相似） '''\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def l2_distance(A, B):\n",
    "    ''' 欧式距离（越小越相似） '''\n",
    "    x = np.asarray(A)\n",
    "    y = np.asarray(B)\n",
    "    return norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入（Embedding）模型\n",
    "\n",
    "[嵌入模型库](https://www.modelscope.cn/models?page=1&tasks=sentence-embedding&type=nlp)\n",
    "\n",
    "标准\n",
    "\n",
    "- **找需求相关的语料库来进行文本向量转换测试，进行评估**\n",
    "- 向量模型的精确度直接影响相似度检索的文档召回率\n",
    "- 大多数场景下，开源的嵌入模型使用效果都一般，要进行检索召回率，建议对模型进行微调\n",
    "- 嵌入模型的维度越高，表示特征细节提取越丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dimension: 1024\n",
      "Fist 10 Dimensions: [-0.005723029375076294, 0.04400135949254036, 0.019950373098254204, 0.022535542026162148, -0.011223198845982552, 0.016607481986284256, 0.04036429524421692, 0.03075459785759449, 0.015653643757104874, 0.03501566872000694]\n",
      "\n",
      "[ARK] Total Dimension: 4096\n",
      "[ARK] Fist 10 Dimensions: [0.2421875, 3.859375, -0.482421875, -3.96875, 0.74609375, -2.625, 1.6015625, 3.359375, 0.3984375, 3.25]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "def get_embeddings(texts: List[str]):\n",
    "    ''' OpenAI Embeddings '''\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        base_url=os.getenv('OPENAI_BASE_URL'),\n",
    "        model=os.getenv('EMBEDDING_MODEL_NAME')\n",
    "    )\n",
    "    return embedding_model.embed_documents(texts)\n",
    "\n",
    "def get_ark_embeddings(texts: List[str]):\n",
    "    client = Ark(api_key=os.getenv('ARK_API_KEY'))\n",
    "    return client.embeddings.create(\n",
    "        model=\"doubao-embedding-large-text-240915\",\n",
    "        input=texts\n",
    "    )\n",
    "\n",
    "# def get_embeddings(texts, model=\"text-embedding-ada-002\", dimensions=None):\n",
    "#     '''封装 OpenAI 的 Embedding 模型接口'''\n",
    "#     if model == \"text-embedding-ada-002\":\n",
    "#         dimensions = None\n",
    "#     if dimensions:\n",
    "#         data = client.embeddings.create(\n",
    "#             input=texts, model=model, dimensions=dimensions).data\n",
    "#     else:\n",
    "#         data = client.embeddings.create(input=texts, model=model).data\n",
    "#     return [x.embedding for x in data]\n",
    "\n",
    "test_texts = ['吃完海鲜可以喝牛奶吗？']\n",
    "vec = get_embeddings(test_texts)[0]\n",
    "print(f'Total Dimension: {len(vec)}')\n",
    "print(f'Fist 10 Dimensions: {vec[:10]}')\n",
    "\n",
    "vec = get_ark_embeddings(test_texts).data[0].embedding\n",
    "print(f'\\n[ARK] Total Dimension: {len(vec)}')\n",
    "print(f'[ARK] Fist 10 Dimensions: {vec[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 与自己的余弦距离为：1.0\n",
      "query 与文档的余弦距离为：\n",
      "0.8854540946810185\n",
      "0.8605012890827118\n",
      "0.80269178686373\n",
      "0.7593635176801089\n",
      "0.7953524476835953\n",
      "********************************************************************************\n",
      "query 与自己的L2距离为：0.0\n",
      "query 与文档的L2距离为：\n",
      "63.82647284306631\n",
      "70.09578368205798\n",
      "82.47880162653664\n",
      "91.39691180339793\n",
      "83.86019628342954\n"
     ]
    }
   ],
   "source": [
    "query = \"国际争端\"\n",
    "# 前两条为国际争端\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本歧阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\"\n",
    "]\n",
    "\n",
    "# query_vector = get_embeddings([query])[0]\n",
    "# doc_vectors = get_embeddings(documents)\n",
    "\n",
    "# 实测 doubao Embedding 模型效果更好\n",
    "query_vector = get_ark_embeddings([query]).data[0].embedding\n",
    "doc_vectors = [doc_vector.embedding for doc_vector in get_ark_embeddings(documents).data]\n",
    "\n",
    "# 余弦距离越大表示越相似\n",
    "print(f'query 与自己的余弦距离为：{cosine_similarity(query_vector, query_vector)}')\n",
    "print('query 与文档的余弦距离为：')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{cosine_similarity(query_vector, doc_vector)}')\n",
    "\n",
    "print('*' * 80)\n",
    "\n",
    "# L2距离（欧式距离）越小表示越相似\n",
    "print(f'query 与自己的L2距离为：{l2_distance(query_vector, query_vector)}')\n",
    "print('query 与文档的L2距离为：')\n",
    "for doc_vector in doc_vectors:\n",
    "    print(f'{l2_distance(query_vector, doc_vector)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量数据库 Chroma\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "官方文档：https://docs.trychroma.com/docs/overview/introduction<br>\n",
    "<b>扩展阅读：</b>https://guangzhengli.com/blog/zh/vector-database\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>澄清几个关键概念：</b><ul>\n",
    "    <li>向量数据库的意义是快速的检索；</li>\n",
    "    <li>向量数据库本身不生成向量，向量是由 Embedding 模型产生的；</li>\n",
    "    <li>向量数据库与传统的关系型数据库是互补的，不是替代关系，在实际应用中根据实际需求经常同时使用。</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "医学的发展不能脱离它所处的时代。医学思想和实践来自与之相适应的知识环境，同时又为拓展和丰富人类的知识\n",
      "\n",
      "贡献力量。从原始社会到现在几千年的发展历程，医学的发展道路艰难曲折，不仅囊括了医学的各门学科，而且还\n",
      "\n",
      "涉及丰富多彩的人类医疗卫生活动。医学的发展凝聚着一代又一代先行者的心血和智慧，它既是人类对自身疾病与\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "\n",
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):\n",
    "    \"\"\"Extract text from a PDF file.\n",
    "    Args:\n",
    "        filename (str): Path to the PDF file.\n",
    "        page_numbers (list[int], optional): List of page numbers to extract. If None, extract all pages.\n",
    "        min_line_length (int, optional): Minimum length of a line to be included in the output.\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    buffer, full_text = '', ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    for line in full_text.split('\\n'):\n",
    "        if len(line) >= min_line_length:\n",
    "            buffer += (' ' + line) if not line.endswith('-') else line.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer.strip())\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer.strip())\n",
    "    return paragraphs  \n",
    "\n",
    "# pdfminer 的效果不太好，这里只演示效果，实际用 markdown 文件\n",
    "paragraphs = extract_text_from_pdf(\"file/医学史.pdf\", min_line_length=45)\n",
    "for paragraph in paragraphs[:3]:\n",
    "    print(paragraph + '\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了演示方便，我们只取两页（第一章/Page 3、Page 4）\n",
    "# paragraphs = extract_text_from_pdf(\n",
    "#     \"file/医学史.pdf\",\n",
    "#     page_numbers=[3, 3],\n",
    "#     min_line_length=10\n",
    "# )\n",
    "\n",
    "# for paragraph in paragraphs:\n",
    "#     print(paragraph + '\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割/检索 markdown 文件\n",
    "\n",
    "**文本分割的粒度**  \n",
    "1. 粒度太大可能导致检索不精准，粒度太小可能导致信息不全面\n",
    "2. 问题的答案可能跨越两个片段\n",
    "\n",
    "LangChain 文本分割的策略：\n",
    "1. **基于字符的分割（`CharacterTextSplitter`）**\n",
    "\n",
    "    按照指定的字符进行分割，默认使用换行符和空格作为分隔符。通过设置 `chunk_size` 和 `chunk_overlap` 参数，可以控制每个文本块的大小和重叠部分。\n",
    "\n",
    "    ```python\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "    text = \"这是一段示例文本，用于演示文本分割的策略。\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\" \",\n",
    "        chunk_size=10,\n",
    "        chunk_overlap=2\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(chunks)\n",
    "    ```\n",
    "\n",
    "2. **基于 token 的分割（`TokenTextSplitter`）**\n",
    "\n",
    "    `TokenTextSplitter` 基于 `token` 进行分割，而不是字符。标记是自然语言处理中对文本进行分词后的基本单元。通过设置 `chunk_size 和 `chunk_overlap` 参数，可以控制每个文本块包含的标记数量。\n",
    "\n",
    "\n",
    "3. **按文档结构分割（`MarkdownTextSplitter`、`HtmlTextSplitter`）**\n",
    "\n",
    "    `MarkdownTextSplitter` 专门用于分割 Markdown 文档，它会根据 Markdown 的语法结构（如标题、列表等）进行分割，确保分割后的文本块具有一定的语义完整性。  \n",
    "    `HtmlTextSplitter` 用于分割 HTML 文档，它会根据 HTML 的标签结构进行分割，去除 HTML 标签，只保留文本内容，并将文本分割成合适的块。\n",
    "\n",
    "4. **递归分割（`RecursiveCharacterTextSplitter`）**\n",
    "\n",
    "    它会尝试按照多个分隔符进行分割，优先使用较长的分隔符，以确保分割后的文本块具有较好的语义完整性。如果使用较长的分隔符无法满足 chunk_size 的要求，则会尝试使用较短的分隔符。\n",
    "\n",
    "    ```python\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    text = \"这是一段示例文本，用于演示递归文本分割的策略。\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"。\", \"，\", \" \"],\n",
    "        chunk_size=10,\n",
    "        chunk_overlap=2\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(chunks)\n",
    "    ```\n",
    "\n",
    "5. **基于语义分割**\n",
    "\n",
    "    1. 根据句子、段落或主题等有语义的单元进行分割；\n",
    "    2. 为每个片段创建 Embeddig；\n",
    "    3. 对相邻的片段计算余弦相似度，选择相似度最高的两个片段进行合并，直到余弦相似度显著下降位置。\n",
    "\n",
    "    > 或者使用早期预训练模型（如 BERT）中用于增强句子级语义理解的 NSP 任务（Next Sentence Prediction，下一句预测任务） 方法。\n",
    "\n",
    "6. **基于 LLM 分割**\n",
    "\n",
    "    通过 Prompt + LLM 的方式，将文本分割成多个片段，每个片段包含一个语义单元。这种方法将保证较高的语义准确性，因为 LLM 可以更好地理解文本的语义。\n",
    "\n",
    "---\n",
    "\n",
    "在实际应用中，选择合适的文本分割器应根据具体需求进行。例如：\n",
    "- 对于简单文本，可以选择 `CharacterTextSplitter`。\n",
    "- 处理长文本或需要上下文信息的场合，推荐使用` RecursiveCharacterTextSplitter` 或 `TokenTextSplitter`。\n",
    "- 中文文章推荐用 `RecursiveCharacterTextSplitter`\n",
    "- 处理 `Markdown` 文档时，`MarkdownTextSplitter` 是最佳选择，而对于`LaTeX`文档，则应使用`LatexTextSplitter`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "\n",
    "# chunk_size 一般根据文档内容或大小来设置\n",
    "# overlap_size 一般设置 chunk_size 大小的10%-20%之间\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''按指定 chunk_size 和 overlap_size 交叠割文本'''\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # 向前计算重叠部分\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # 向后计算当前chunk\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "此处 sent_tokenize 为针对英文的实现，针对中文的实现请参考 <a herf='./chinese_utils.py'>chinese_utils.py</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** Document 1: *********************************\n",
      "医学的发展不能脱离它所处的时代。医学思想和实践来自与之相适应的知识环境，同时又为拓展和丰富人类的知识贡献力量。从原始社会到现在几千年的发展历程，医学的发展道路艰难曲折，不仅囊括了医学的各门学科，而且还涉及丰富多彩的人类医疗卫生活动。医学的发展凝聚着一代又一代先行者的心血和智慧，它既是人类对自身疾病与健康及其关系的认识史，也是一部伴随着社会生产发展，由经验到科学，由低级到高级、由单一到综合逐渐进化的发展史，既是科学技术进步的一个缩形，也是人类文化史的一个重要组成。\n",
      "\n",
      "## 一、古代医学\n",
      "受古代生产力及科学技术水平的限制，古代医学知识多来源于医疗实践经验的积累，夹杂着唯心主义和迷信思想。宗教和文学在一定程度上促进了古代医学的发展。在数千年的历史长河中，古代医学经历了漫长的发展历程。\n",
      "\n",
      "### （一）史前医学\n",
      "在漫长的 300 至万年前的原始社会中，原始人类在同疾病斗争的过程中逐步认识了各种植物、动物和矿物等的药效，开辟了以经验为起源的各项医药活动。在原始社会末期，已有了断肢术、阉割术，穿颅术、剖宫产术等外科手术和相应的外伤治疗外用药物。这些进步，也与社会生产工具的发明和改进密切相关。\n",
      "************************** Document 2: *********************************\n",
      "### （一）史前医学\n",
      "在漫长的 300 至万年前的原始社会中，原始人类在同疾病斗争的过程中逐步认识了各种植物、动物和矿物等的药效，开辟了以经验为起源的各项医药活动。在原始社会末期，已有了断肢术、阉割术，穿颅术、剖宫产术等外科手术和相应的外伤治疗外用药物。这些进步，也与社会生产工具的发明和改进密切相关。\n",
      "\n",
      "由于原始社会的生产力低下，人们不能正确理解疾病和死亡，认为疾病是由一种超自然的力量所形成和主宰的，认为自然界的一切现象都由一种超自然的实体控制，从而产生了万物有灵的观念：山神、河怪、精灵、疯魔等都可导致人类疾病。在这些认识支配下，产生了招魂、驱鬼、敬神、祷告等病方法，产生了祭司和巫师，也产生了图腾崇拜，医学被打上了原始宗教的烙印。\n",
      "\n",
      "### （二）古代东方医学\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def read_markdown_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：未找到文件 {file_path}。\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"错误：读取文件时发生未知错误 {e}。\")\n",
    "        return None\n",
    "\n",
    "file_path = os.path.abspath(os.path.join(os.getcwd(), 'file/医学史.md'))\n",
    "markdown_content = read_markdown_file(file_path)\n",
    "\n",
    "\n",
    "\n",
    "docs = []\n",
    "if markdown_content:\n",
    "    # 创建 Markdown 文本分割器\n",
    "    # markdown_splitter = MarkdownTextSplitter(chunk_size=512, chunk_overlap=250)\n",
    "    markdown_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=250)\n",
    "\n",
    "    # 分割 Markdown 内容为多个 Document\n",
    "    docs = markdown_splitter.create_documents([markdown_content])\n",
    "\n",
    "    # 打印每个 Document 的内容\n",
    "    for i, doc in enumerate(docs[:2]):\n",
    "        print(f\"************************** Document {i + 1}: *********************************\")\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn, batch_size=None):\n",
    "        # 内存模式\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "        # 数据持久化\n",
    "        # chroma_client = chromadb.PersistentClient(path=\"./chroma\")\n",
    "\n",
    "        # NOTE: 清空数据，为了演示，实际不需要每次 reset()，并且是不可逆的\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(\n",
    "            name=collection_name, \n",
    "            metadata={\"hnsw:space\": \"cosine\"} # l2 is the default\n",
    "        )\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents, batch_size=None):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        n = len(documents)\n",
    "        if batch_size is None:\n",
    "            self.collection.add(\n",
    "                embeddings=self.embedding_fn(documents),        # 每个文档的向量\n",
    "                documents=documents,                            # 文档的原文\n",
    "                ids=[f\"id{i}\" for i in range(n)]   # 每个文档的 id\n",
    "            )\n",
    "        else:\n",
    "            for i in range(0, n, batch_size):\n",
    "                end_idx = min(i + batch_size, n)\n",
    "                self.collection.add(\n",
    "                    embeddings=self.embedding_fn(documents[i:end_idx]),        # 每个文档的向量\n",
    "                    documents=documents[i:end_idx],                            # 文档的原文\n",
    "                    ids=[f\"id{i}\" for i in range(i, end_idx)]   # 每个文档的 id\n",
    "                )\n",
    "        print(f\"\\n 😁 添加{n}份文档成功 😁\\n\")\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** 这里如果提示 `sqlite` 版本过低，可以安装 `pysqlite3-binary`，然后在虚拟环境的 `chromadb/__init__.py` 中添加如下代码：\n",
    "```python\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Chroma 里，`collection.query` 返回结果中的 `distances` 表示查询向量和集合中返回的文档向量之间的距离。一般而言，这个值越小代表两个向量越相似。\n",
    "1. 余弦距离（`distance_metric='cosine'`）\n",
    "含义：余弦距离基于余弦相似度得出，公式为 `余弦距离 = 1 - 余弦相似度`。余弦相似度衡量的是两个向量夹角的余弦值，范围是 [-1, 1]。当两个向量方向完全相同时，余弦相似度为 1，余弦距离为 0；当两个向量方向完全相反时，余弦相似度为 -1，余弦距离为 2。\n",
    "评判标准：余弦距离越接近 0，意味着查询向量和文档向量的方向越相近，也就是两个向量越相似。所以在使用余弦距离时，`distances` 的值越小越好。\n",
    "\n",
    "2. 欧几里得距离（`distance_metric='l2'`）\n",
    "含义：欧几里得距离指的是 n 维空间中两个点之间的直线距离。\n",
    "评判标准：欧几里得距离为 0 时，表示两个向量完全相同；值越大，表明两个向量在空间中的距离越远，相似度越低。所以使用欧几里得距离时，同样是 `distances` 的值越小越好。\n",
    "\n",
    "3. 内积距离（`distance_metric='ip'`）\n",
    "含义：内积距离基于向量的内积来计算。内积反映了两个向量在方向上的一致性和长度的乘积。\n",
    "评判标准：一般来说，内积值越大，两个向量越相似，但在 Chroma 中使用内积作为距离度量时，通常会对其进行转换，使得距离值越小表示越相似。\n",
    "\n",
    "在 Chroma 中，`collection.query` 返回的结果默认是按照 距离从小到大排序的，因此 `distances` 越小的结果会排在越前面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 😁 添加59份文档成功 😁\n",
      "\n",
      "**************** 😁 rank:1 | distance: 0.3314863443374634 *****************\n",
      "1953年，美国分子生物学家洪森 (James Dewey Watson.1928-)和英国物理学家克里克 (FrancisHarry Compton Crick,1916--2004）以及英国物理学家威尔金斯 (Maurice Hugh Frederick Wilkins, 1916--2004) 发现并阐明了 DNA 分子的双螺旋结构，奠定了分子生物学的基础。他们三人分获 1962年诺贝尔生理学或医学奖。1965年，我国科学家在世界上首次用化学方法合成了牛胰岛素；之后，美国科学家也合成了含有206个核苷酸的 DNA 大分子。70年代发现了逆转录酶和限制性内切酶，促进了基因工程的发展。80年代初，临床开始应用基因工程治疗疾病，如用单克隆技术治疗癌症。人们在发酵工业中大量生产胰岛素，保证了临床用药的需要，降低了成本，减轻了病人的经济负担。\n",
      "\n",
      "（2）医学遗传学的发展：\n",
      "\n",
      "医学遗传学是研究人类疾病与遗传的关系，研究人类遗传病形成的机制和遗传方式，以及遗传病的诊断、治疗、预后、复发危险和预防的科学，是医学与遗传学相结合的边缘学科。\n",
      "\n",
      "**************** 😁 rank:2 | distance: 0.33595776557922363 *****************\n",
      "显微镜的应用为 19 世纪细胞学的建立打下了良好的基础。\n",
      "\n",
      "#### 3．医学三学派的成熟\n",
      "\n",
      "在17世纪，由于物理学、化学和生物学的进步，使一些学者主张以单一学科的理论来解释生命现象和病理现象，出现三个学派。\n",
      "\n",
      "（1）物理学派：代表人物是法国数学家、物理学家笛卡尔。他认为：“宇宙是一个庞大的机械，人的身体也是一部精细的机械，从宏观到微观，所有物体无一不可用机械原理来阐明。”身体的一切疼痛、恐怖表现都是机械的反应。伽利略的学生波累利认为肌肉运动是一种力学原理，人心脏的搏动、胃肠 运动都符合力学原理。他甚至认为胃的消化功能就是摩擦力作用的结果。\n",
      "\n",
      "**************** 😁 rank:3 | distance: 0.3737775683403015 *****************\n",
      "17世纪的医学进步得益于伽利略（Galileo Galilei. 1564-1642)和刻卜勒（Jobannes Kepler,1571-1630）等一批杰出科学家的成就。例如帕多瓦大学的教授桑克托留斯（Sanctorius,1561-1636) 所设计的最早的体温计和脉搏计是根据伽利略的发明而加以改制的。\n",
      "\n",
      "#### 1．生理学的进展\n",
      "\n",
      "17世纪初，由于量度的应用，使生命科学的研究步入科学轨道。其标志之一是英国医学家威康•哈维（ Wiliam Harvey,1578-—1657） 发现血液循环，创建了血流循环学说。从而使生理学从解剖学中分立出来。哈维首先应用活体解剖的实验方法，并应用度量的概念，精确地计算出心脏每分钟搏出血量和每小时搏出血量。他于1628年发表了著作心血运动论）(The Movement of the heart and the Blood), 标志着血液循环理论的建立。恩格斯对哈维的发现做出这样的评价：“由于哈维发现血液循环，而把生理学确立为一门科学。”生理学家巴甫洛夫 (Ivan Pavlov,1849—1936）也评价说；“哈维的研究为动物生理学奠定了基础。”\n",
      "\n",
      "**************** 😁 rank:4 | distance: 0.4203561544418335 *****************\n",
      "意大利人马尔匹基用显微镜观察了动、植物的微细构造，开拓了组织学分野。18世纪末，研究个体发生的胚胎学开始起步。德国胚胎学家冯贝尔（Karl Emst Von Bear.1792-1876）提出“胚层说”，认为除极低等的动物外，一切动物最终由胚层发育成动物器官。《动物的发育》是他出版的胚胎学专著。\n",
      "\n",
      "19世纪意大利学者高尔基（Camello Golgi, 1843-1926)首创镀银浸染神经元技术，被称为神经解剖学创始人之一。\n",
      "\n",
      "#### 3．药理学\n",
      "\n",
      "19世纪化学技术的进步使提取药用的植物有效成分成为可能，例如 1806 年从鸦片中提取出吗啡；1817 年从吐根中提取出叶根碱；1818 年从马钱子中提取士的宁；1821 年从咖啡中提取出咖啡因等；1826年从金鸡纳树皮提取出奎宁。19世纪初，在德国建立了第一个药理实验室，出版了第一本药理教科书，标志着独立的药理学科的建立。19世纪中叶，已能人工合成一次药物，如人工合成尿素、氯仿、苯胺、硫酸盐类解热镇痛剂等。人们以临床医学和生理学为基础，以动物实验为手段，开始探讨药物的作用及其机制，从而建立了实验药理学。\n",
      "\n",
      "#### 4．病理生理学\n",
      "\n",
      "**************** 😁 rank:5 | distance: 0.620252788066864 *****************\n",
      "我国现代临床医学在基础医学、预防医学、药学的协同支撑下，在物理诊断学、实验诊断学、传染病学与寄生虫病学、内科学、地方病学、外科学、妇产科学、儿科学、眼科学、耳鼻咽喉科学、皮肤性病学、口腔医学、精神病学、神经病学、营养与食品卫生学、放射医学、护理学、临床肿瘤学、核医学等方面，均取得了显著的发展，学科体系齐全，技术装备先进，技术水平提高，国民健康得到有力保障。\n",
      "\n",
      "### （二）21世纪医学的进展趋势\n",
      "\n",
      "21 世纪的医学将进入高科技时代，医学的理论和技术将有更大更深入的发展，从根本上解除最严重疾病对人类的威胁。分子生物学、系统生物学与生物医学、预防医学、转化医学、个体化医学、精准医学、医学整合等领域将是21世纪医学发展的优先领域。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 提取文档的文本内容\n",
    "document_texts = [doc.page_content for doc in docs]\n",
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# 向向量数据库中添加文档\n",
    "vector_db.add_documents(document_texts)\n",
    "\n",
    "user_query = \"\"\"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道\"\"\"\n",
    "results = vector_db.search(user_query, 5)\n",
    "\n",
    "for idx, para in enumerate(results['documents'][0]):\n",
    "    print(f\"**************** 😁 rank:{idx +1} | distance: {results['distances'][0][idx]} *****************\")\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于向量检索的RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=3):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, context=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "量度的应用使生命科学的研究步入科学轨道。\n"
     ]
    }
   ],
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(vector_db,llm_api=get_completion)\n",
    "\n",
    "user_query = \"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道?\"\n",
    "\n",
    "response = bot.chat(user_query)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索后排序（rerank）\n",
    "\n",
    "从向量数据库里召回后，有时候相关性高的文档不一定排在前面，需要根据相关性排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** rank:1 | distance: 0.3314863443374634 *****************\n",
      "1953年，美国分子生物学家洪森 (James Dewey Watson.1928-)和英国物理学家克里克 (FrancisHarry Compton Crick,1916--2004）以及英国物理学家威尔金斯 (Maurice Hugh Frederick Wilkins, 1916--2004) 发现并阐明了 DNA 分子的双螺旋结构，奠定了分子生物学的基础。他们三人分获 1962年诺贝尔生理学或医学奖。1965年，我国科学家在世界上首次用化学方法合成了牛胰岛素；之后，美国科学家也合成了含有206个核苷酸的 DNA 大分子。70年代发现了逆转录酶和限制性内切酶，促进了基因工程的发展。80年代初，临床开始应用基因工程治疗疾病，如用单克隆技术治疗癌症。人们在发酵工业中大量生产胰岛素，保证了临床用药的需要，降低了成本，减轻了病人的经济负担。\n",
      "\n",
      "（2）医学遗传学的发展：\n",
      "\n",
      "医学遗传学是研究人类疾病与遗传的关系，研究人类遗传病形成的机制和遗传方式，以及遗传病的诊断、治疗、预后、复发危险和预防的科学，是医学与遗传学相结合的边缘学科。\n",
      "\n",
      "**************** rank:2 | distance: 0.33595776557922363 *****************\n",
      "显微镜的应用为 19 世纪细胞学的建立打下了良好的基础。\n",
      "\n",
      "#### 3．医学三学派的成熟\n",
      "\n",
      "在17世纪，由于物理学、化学和生物学的进步，使一些学者主张以单一学科的理论来解释生命现象和病理现象，出现三个学派。\n",
      "\n",
      "（1）物理学派：代表人物是法国数学家、物理学家笛卡尔。他认为：“宇宙是一个庞大的机械，人的身体也是一部精细的机械，从宏观到微观，所有物体无一不可用机械原理来阐明。”身体的一切疼痛、恐怖表现都是机械的反应。伽利略的学生波累利认为肌肉运动是一种力学原理，人心脏的搏动、胃肠 运动都符合力学原理。他甚至认为胃的消化功能就是摩擦力作用的结果。\n",
      "\n",
      "**************** rank:3 | distance: 0.3737775683403015 *****************\n",
      "17世纪的医学进步得益于伽利略（Galileo Galilei. 1564-1642)和刻卜勒（Jobannes Kepler,1571-1630）等一批杰出科学家的成就。例如帕多瓦大学的教授桑克托留斯（Sanctorius,1561-1636) 所设计的最早的体温计和脉搏计是根据伽利略的发明而加以改制的。\n",
      "\n",
      "#### 1．生理学的进展\n",
      "\n",
      "17世纪初，由于量度的应用，使生命科学的研究步入科学轨道。其标志之一是英国医学家威康•哈维（ Wiliam Harvey,1578-—1657） 发现血液循环，创建了血流循环学说。从而使生理学从解剖学中分立出来。哈维首先应用活体解剖的实验方法，并应用度量的概念，精确地计算出心脏每分钟搏出血量和每小时搏出血量。他于1628年发表了著作心血运动论）(The Movement of the heart and the Blood), 标志着血液循环理论的建立。恩格斯对哈维的发现做出这样的评价：“由于哈维发现血液循环，而把生理学确立为一门科学。”生理学家巴甫洛夫 (Ivan Pavlov,1849—1936）也评价说；“哈维的研究为动物生理学奠定了基础。”\n",
      "\n",
      "**************** rank:4 | distance: 0.4203561544418335 *****************\n",
      "意大利人马尔匹基用显微镜观察了动、植物的微细构造，开拓了组织学分野。18世纪末，研究个体发生的胚胎学开始起步。德国胚胎学家冯贝尔（Karl Emst Von Bear.1792-1876）提出“胚层说”，认为除极低等的动物外，一切动物最终由胚层发育成动物器官。《动物的发育》是他出版的胚胎学专著。\n",
      "\n",
      "19世纪意大利学者高尔基（Camello Golgi, 1843-1926)首创镀银浸染神经元技术，被称为神经解剖学创始人之一。\n",
      "\n",
      "#### 3．药理学\n",
      "\n",
      "19世纪化学技术的进步使提取药用的植物有效成分成为可能，例如 1806 年从鸦片中提取出吗啡；1817 年从吐根中提取出叶根碱；1818 年从马钱子中提取士的宁；1821 年从咖啡中提取出咖啡因等；1826年从金鸡纳树皮提取出奎宁。19世纪初，在德国建立了第一个药理实验室，出版了第一本药理教科书，标志着独立的药理学科的建立。19世纪中叶，已能人工合成一次药物，如人工合成尿素、氯仿、苯胺、硫酸盐类解热镇痛剂等。人们以临床医学和生理学为基础，以动物实验为手段，开始探讨药物的作用及其机制，从而建立了实验药理学。\n",
      "\n",
      "#### 4．病理生理学\n",
      "\n",
      "**************** rank:5 | distance: 0.620252788066864 *****************\n",
      "我国现代临床医学在基础医学、预防医学、药学的协同支撑下，在物理诊断学、实验诊断学、传染病学与寄生虫病学、内科学、地方病学、外科学、妇产科学、儿科学、眼科学、耳鼻咽喉科学、皮肤性病学、口腔医学、精神病学、神经病学、营养与食品卫生学、放射医学、护理学、临床肿瘤学、核医学等方面，均取得了显著的发展，学科体系齐全，技术装备先进，技术水平提高，国民健康得到有力保障。\n",
      "\n",
      "### （二）21世纪医学的进展趋势\n",
      "\n",
      "21 世纪的医学将进入高科技时代，医学的理论和技术将有更大更深入的发展，从根本上解除最严重疾病对人类的威胁。分子生物学、系统生物学与生物医学、预防医学、转化医学、个体化医学、精准医学、医学整合等领域将是21世纪医学发展的优先领域。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"\"\"17世纪初，由于什么的应用，使生命科学的研究步入科学轨道\"\"\"\n",
    "results = vector_db.search(user_query, 5)\n",
    "\n",
    "# 这里可以看出，最相关的文档为第3个文档\n",
    "for idx, para in enumerate(results['documents'][0]):\n",
    "    print(f\"**************** rank:{idx +1} | distance: {results['distances'][0][idx]} *****************\")\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "import requests\n",
    "\n",
    "class Rerank():\n",
    "    def __init__(self, model: str, base_url: str, api_key: str):\n",
    "        self.rerank_model = model,\n",
    "        self.base_url = base_url,\n",
    "        self.api_key = api_key\n",
    "        print(f'rerank 初始化成功：{self.rerank_model}, {self.base_url}, {self.api_key}')\n",
    "\n",
    "    def rerank_cloud(self, results: List[str], query: str, k=10) -> List[Document]:\n",
    "        \"\"\"\n",
    "        使用云端重排序模型模型对检索结果进行重排序\n",
    "\n",
    "        :param results: 原始检索结果\n",
    "        :param query: 查询\n",
    "        :param k: 返回的top-k结果数\n",
    "        :return: 重排序后的结果\n",
    "        \"\"\"\n",
    "        # texts = [item.page_content for item in results]\n",
    "        texts = results\n",
    "\n",
    "        url = self.base_url\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.rerank_model,\n",
    "            \"query\": query,\n",
    "            \"documents\": texts,\n",
    "            \"top_n\": k,\n",
    "            \"return_documents\": False,\n",
    "            \"max_chunks_per_doc\": 1024,\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "        try:\n",
    "            if response.status_code == 200:\n",
    "                response = response.json()\n",
    "                indices = [item['index'] for item in response['results']]\n",
    "                return [results[i] for i in indices]\n",
    "        except:\n",
    "            print(f'Error:network error status_code={response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rerank 初始化成功：('c',), ('h',), sk-hybehttizlquaobtbilikijqmuuyzxizjhkfqqlpkkvcvojw\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL \"('h',)\": No scheme supplied. Perhaps you meant https://('h',)?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMissingSchema\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m rerank = Rerank(\n\u001b[32m      2\u001b[39m     model=os.getenv(\u001b[33m'\u001b[39m\u001b[33mRERANK_MODEL_NAME\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m      3\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.siliconflow.cn/v1/rerank\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     api_key=os.getenv(\u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m rerank_results = \u001b[43mrerank\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrerank_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(rerank_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mRerank.rerank_cloud\u001b[39m\u001b[34m(self, results, query, k)\u001b[39m\n\u001b[32m     26\u001b[39m payload = {\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.rerank_model,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_chunks_per_doc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1024\u001b[39m,\n\u001b[32m     33\u001b[39m }\n\u001b[32m     34\u001b[39m headers = {\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.api_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Agent/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Agent/.venv/lib/python3.11/site-packages/requests/sessions.py:575\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[32m    563\u001b[39m req = Request(\n\u001b[32m    564\u001b[39m     method=method.upper(),\n\u001b[32m    565\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m     hooks=hooks,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m prep = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m proxies = proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    579\u001b[39m settings = \u001b[38;5;28mself\u001b[39m.merge_environment_settings(\n\u001b[32m    580\u001b[39m     prep.url, proxies, stream, verify, cert\n\u001b[32m    581\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Agent/.venv/lib/python3.11/site-packages/requests/sessions.py:484\u001b[39m, in \u001b[36mSession.prepare_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    481\u001b[39m     auth = get_netrc_auth(request.url)\n\u001b[32m    483\u001b[39m p = PreparedRequest()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Agent/.venv/lib/python3.11/site-packages/requests/models.py:367\u001b[39m, in \u001b[36mPreparedRequest.prepare\u001b[39m\u001b[34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_method(method)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_headers(headers)\n\u001b[32m    369\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_cookies(cookies)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Agent/.venv/lib/python3.11/site-packages/requests/models.py:438\u001b[39m, in \u001b[36mPreparedRequest.prepare_url\u001b[39m\u001b[34m(self, url, params)\u001b[39m\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(*e.args)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[32m    439\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No scheme supplied. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    440\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    441\u001b[39m     )\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m: No host supplied\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mMissingSchema\u001b[39m: Invalid URL \"('h',)\": No scheme supplied. Perhaps you meant https://('h',)?"
     ]
    }
   ],
   "source": [
    "rerank = Rerank(\n",
    "    model=os.getenv('RERANK_MODEL_NAME', 'cross-encoder/ms-marco-MiniLM-L-6-v2'),\n",
    "    base_url=\"https://api.siliconflow.cn/v1/rerank\",\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "rerank_results = rerank.rerank_cloud(results, user_query, k=5)\n",
    "print(rerank_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
