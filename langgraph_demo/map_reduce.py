import operator
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.env_util import *
from langgraph_utils.common_util import gen_mermaid
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from typing import Annotated
from langgraph.types import Send
from pydantic import BaseModel, Field

# å®šä¹‰ Prompt
subjects_prompt = """åé¢çš„å¯¹è¯éƒ½ä½¿ç”¨ä¸­æ–‡æ¥å›ç­”ï¼Œéšæœºç”Ÿæˆ4ä¸ªä¸ {topic} ç›¸å…³çš„å…³é”®å­—ï¼Œæ³¨æ„åªè¦4ä¸ªï¼Œä¸è¦å¤šä¹Ÿä¸è¦å°‘ï¼Œä¸è¦é‡å¤ï¼Œç”Ÿæˆåä½ éœ€è¦è‡ªå·±æ£€æŸ¥ç»“æœæ˜¯å¦æ­£ç¡®ã€‚"""
joke_prompt = """ç”Ÿæˆä¸€æ¡å…³äº {subject} çš„ç¬‘è¯ï¼Œä½¿ç”¨ä¸­æ–‡ï¼Œåªè¿”å›ä¸€è¡Œæ–‡å­—ï¼Œä¸è¦è¿”å›å¤šè¡Œã€‚ä¸è¦ä½¿ç”¨\\nç­‰ç‰¹æ®Šå­—ç¬¦ï¼Œæœ€åä½ éœ€è¦æ£€æŸ¥ç»“æœæ˜¯å¦æ»¡è¶³è¦æ±‚"""
best_joke_prompt = """ä¸‹é¢æ˜¯4è¡Œæ˜¯4ä¸ªå…³äº {topic} çš„ç¬‘è¯ï¼Œä»å‰åˆ°åIDåˆ†åˆ«æ˜¯0ã€1ã€2ã€3ï¼Œé€‰æ‹©å…¶ä¸­ä¸€ä¸ªå¹¶è¿”å›å…¶IDï¼Œä¸è¦å¤šä½™çš„åˆ†ææˆ–æè¿°ï¼Œæœ€ç»ˆç»“æœåªè¦è¿”å›ä¸€ä¸ªçº¯æ•°å­—ã€‚
{jokes}"""

class Subjects(BaseModel):
    subjects: list[str]

class Joke(BaseModel):
    joke: str

class BestJoke(BaseModel):
    id: int = Field(description="åºå·ï¼Œintç±»å‹ï¼Œå¦‚ï¼š1", ge=0)

model = ChatOpenAI(
    openai_api_key=get_openai_api_key(),
    # model_name=get_default_model(),
    model_name="THUDM/GLM-Z1-32B-0414",
    base_url=get_openai_base_url(),
    temperature=0.0,
)

class OverallState(TypedDict):
    topic: str
    subjects: Annotated[list, operator.add]
    jokes: Annotated[list, operator.add]
    best_selected_joke: str


# ç¬‘è¯çš„ subject
class JokeState(TypedDict):
    subject: str

# é€šè¿‡ä¸€ä¸ª topic ç”Ÿæˆå¤šä¸ª subject
def generate_topics(state: OverallState):
    prompt = subjects_prompt.format(topic=state["topic"])
    print(f'ğŸ‘¨ {prompt}')
    response = model.with_structured_output(Subjects).invoke(prompt)
    print(f"âš™ï¸ç”Ÿæˆä¸»é¢˜ï¼š{response.subjects}")
    return {"subjects": response.subjects}

def continue_to_jokes(state: OverallState):
    """
        è¿”å›ä¸€ä¸ª `Send` å¯¹è±¡åˆ—è¡¨
        æ¯ä¸ª `Send` å¯¹è±¡ç”±å›¾ä¸­èŠ‚ç‚¹çš„åç§°ç»„æˆ
        ä»¥åŠå‘é€åˆ°è¯¥èŠ‚ç‚¹çš„çŠ¶æ€

        è¿™é‡Œæ˜¯æŠŠæ‰€æœ‰ç”Ÿæˆçš„ subject éƒ½å‘é€ç»™ `generate_joke` ç”Ÿæˆå¯¹åº”ä¸»é¢˜çš„ç¬‘è¯
    """
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

# ç”Ÿæˆä¸€æ¡ç¬‘è¯
def generate_joke(joke: JokeState) -> OverallState:
    subject = joke["subject"]
    prompt = joke_prompt.format(subject=subject)
    response = model.with_structured_output(Joke).invoke(prompt)
    print(f"âš™ï¸ç”Ÿæˆ[{subject}]ç¬‘è¯ï¼š{response.joke}")
    return {"jokes": [response.joke]}

def best_joke(state: OverallState) -> OverallState:
    """
        ä»å¤šä¸ªç¬‘è¯ä¸­æ‰¾å‡º1ä¸ªæœ€å¥½çš„
    """
    jokes = "\n".join(state["jokes"])
    prompt = best_joke_prompt.format(topic=state["topic"], jokes=jokes)

    print("=" * 80)
    print(f'ğŸ‘¨ {prompt}')
    print("=" * 80)

    response = model.with_structured_output(BestJoke).invoke(prompt)
    print(f"âš™ï¸é€‰æ‹©å‡ºäº†æœ€å¥½çš„ç¬‘è¯ï¼š{response}")
    return {"best_selected_joke": state["jokes"][response.id]}


graph = StateGraph(OverallState)
graph.add_node("generate_topics", generate_topics)
graph.add_node("generate_joke", generate_joke)
graph.add_node("best_joke", best_joke)
graph.add_edge(START, "generate_topics")
graph.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
graph.add_edge("generate_joke", "best_joke")
graph.add_edge("best_joke", END)
app = graph.compile()

# gen_mermaid(app, "map_reduce.mmd")

for s in app.stream({"topic": "åŠ¨ç‰©"}):
    print(s)