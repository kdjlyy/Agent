{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:25:59.007662Z",
     "start_time": "2025-04-24T17:25:58.495328Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from utils.env_util import *\n",
    "from langgraph_utils.common_util import gen_mermaid\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:36:12.254003Z",
     "start_time": "2025-04-24T17:36:12.199792Z"
    }
   },
   "source": [
    "# å®šä¹‰ Prompt\n",
    "# subjects_prompt = \"\"\"éšæœºç”Ÿæˆ4ä¸ªä¸Ž {topic} ç›¸å…³çš„å…³é”®å­—\"\"\"\n",
    "# joke_prompt = \"\"\"ç”Ÿæˆä¸€æ¡å…³äºŽ {subject} çš„ç¬‘è¯\"\"\"\n",
    "# best_joke_prompt = \"\"\"ä¸‹é¢æ˜¯ä¸€äº›å…³äºŽ {topic} çš„ç¬‘è¯ï¼Œé€‰æ‹©æœ€å¥½çš„ä¸€ä¸ªï¼Œè¿”å›žå…¶IDï¼ˆID ä»Ž0å¼€å§‹ï¼‰ã€‚\n",
    "\n",
    "# {jokes}\"\"\"\n",
    "\n",
    "subjects_prompt = \"\"\"Generate a comma separated list of between 2 and 5 examples related to: {topic}.\"\"\"\n",
    "joke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\n",
    "best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one.\n",
    "\n",
    "{jokes}\"\"\"\n",
    "\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int = Field(description=\"Index of the best joke, starting with 0\", ge=0)\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=get_openai_api_key(),\n",
    "    model_name=get_default_model(),\n",
    "    base_url=get_openai_base_url(),\n",
    ")\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str\n",
    "\n",
    "\n",
    "# ç¬‘è¯çš„ subject\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "# é€šè¿‡ä¸€ä¸ª topic ç”Ÿæˆå¤šä¸ª subject\n",
    "def generate_topics(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    print(f\"âš™ï¸ç”Ÿæˆä¸»é¢˜ï¼š{response.subjects}\")\n",
    "    return {\"subjects\": response.subjects}\n",
    "\n",
    "\n",
    "# ç”Ÿæˆä¸€æ¡ç¬‘è¯\n",
    "def generate_joke(state: JokeState):\n",
    "    subject = state[\"subject\"]\n",
    "    prompt = joke_prompt.format(subject=subject)\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    print(f\"âš™ï¸ç”Ÿæˆ[{subject}]ç¬‘è¯ï¼š{response.joke}\")\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    \"\"\"\n",
    "        è¿”å›žä¸€ä¸ª `Send` å¯¹è±¡åˆ—è¡¨\n",
    "        æ¯ä¸ª `Send` å¯¹è±¡ç”±å›¾ä¸­èŠ‚ç‚¹çš„åç§°ç»„æˆ\n",
    "        ä»¥åŠå‘é€åˆ°è¯¥èŠ‚ç‚¹çš„çŠ¶æ€\n",
    "\n",
    "        è¿™é‡Œæ˜¯æŠŠæ‰€æœ‰ç”Ÿæˆçš„ subject éƒ½å‘é€ç»™ `generate_joke`\n",
    "    \"\"\"\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "\n",
    "def best_joke(state: OverallState):\n",
    "    \"\"\"\n",
    "        ä»Žå¤šä¸ªç¬‘è¯ä¸­æ‰¾å‡º1ä¸ªæœ€å¥½çš„\n",
    "    \"\"\"\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}\n",
    "\n",
    "\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "graph.add_edge(START, \"generate_topics\")\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "app = graph.compile()\n",
    "\n",
    "gen_mermaid(app, \"map_reduce.mmd\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™ˆ OPENAI_API_KEY: sk-hybehtt*******************************lpkkvcvojw\n",
      "ðŸ‘€ DEFAULT_MODEL: Qwen/QwQ-32B\n",
      "ðŸ‘€ OPENAI_BASE_URL: https://api.siliconflow.cn/v1\n",
      "âœï¸ å·²ç”Ÿæˆ mermaid æ–‡ä»¶ /Users/yuki/codes/pythonProject/Agent/langgraph_demo/resources/map_reduce.mmd\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:26:54.506846Z",
     "start_time": "2025-04-24T17:26:30.698442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ç”Ÿæˆä¸»é¢˜ï¼š['World Wide Web', 'Component', 'Frame', 'Applet', 'Java Script']\n",
      "{'generate_topics': {'subjects': ['World Wide Web', 'Component', 'Frame', 'Applet', 'Java Script']}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtopic\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43manimal\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2377\u001B[39m, in \u001B[36mPregel.stream\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001B[39m\n\u001B[32m   2371\u001B[39m     \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[32m   2372\u001B[39m     \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001B[39;00m\n\u001B[32m   2373\u001B[39m     \u001B[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[32m   2374\u001B[39m     \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[32m   2375\u001B[39m     \u001B[38;5;66;03m# with channel updates applied only at the transition between steps.\u001B[39;00m\n\u001B[32m   2376\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m loop.tick(input_keys=\u001B[38;5;28mself\u001B[39m.input_channels):\n\u001B[32m-> \u001B[39m\u001B[32m2377\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2378\u001B[39m \u001B[43m            \u001B[49m\u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2379\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2380\u001B[39m \u001B[43m            \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2381\u001B[39m \u001B[43m            \u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2382\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2383\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# emit output\u001B[39;49;00m\n\u001B[32m   2384\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield from\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2385\u001B[39m \u001B[38;5;66;03m# emit output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/codes/pythonProject/Agent/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:223\u001B[39m, in \u001B[36mPregelRunner.tick\u001B[39m\u001B[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[39m\n\u001B[32m    221\u001B[39m end_time = timeout + time.monotonic() \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(futures) > (\u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m get_waiter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     done, inflight = \u001B[43mconcurrent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_when\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconcurrent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m.\u001B[49m\u001B[43mFIRST_COMPLETED\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_time\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmonotonic\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mend_time\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    228\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m    229\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m  \u001B[38;5;66;03m# timed out\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:305\u001B[39m, in \u001B[36mwait\u001B[39m\u001B[34m(fs, timeout, return_when)\u001B[39m\n\u001B[32m    301\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001B[32m    303\u001B[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001B[32m--> \u001B[39m\u001B[32m305\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fs:\n\u001B[32m    307\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m f._condition:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/threading.py:629\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    627\u001B[39m signaled = \u001B[38;5;28mself\u001B[39m._flag\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[32m--> \u001B[39m\u001B[32m629\u001B[39m     signaled = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cond\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    630\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.11.12-macos-aarch64-none/lib/python3.11/threading.py:327\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[32m    326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m         \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    328\u001B[39m         gotit = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    329\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "for s in app.stream({\"topic\": \"animal\"}):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
