from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage
from langchain_openai import ChatOpenAI
import streamlit as st
from dotenv import load_dotenv
import os
import time

OPENAI_API_KEY = None
OPENAI_BASE_URL = None
MODEL = None

class StreamHandler(BaseCallbackHandler):
    """ç»§æ‰¿ BaseCallbackHandlerï¼Œé€šè¿‡ on_llm_new_token æ–¹æ³•æ•è·æ¯ä¸ªæ–°ç”Ÿæˆçš„ token å¹¶å®æ—¶æ›´æ–°ç•Œé¢"""
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

@st.cache_data
def load_env_vars():
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", None)
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", None)
    AVAILABLE_MODELS = os.getenv("AVAILABLE_MODELS", None)
    MODEL_LIST = AVAILABLE_MODELS.split(",") if AVAILABLE_MODELS else []
    CURRENT_MODEL = None
    
    if not OPENAI_API_KEY:
        raise ValueError("Please set OPENAI_API_KEY in your environment variables.")
    else:
        print(f'âœ”ï¸ OPENAI_API_KEY: {OPENAI_API_KEY}')

    if not OPENAI_BASE_URL:
        raise ValueError("Please set OPENAI_BASE_URL in your environment variables.")
    else:
        print(f'âœ”ï¸ OPENAI_BASE_URL: {OPENAI_BASE_URL}')

    if not AVAILABLE_MODELS:
        raise ValueError("Please set AVAILABLE_MODELS in your environment variables.")
    else:
        print(f'âœ”ï¸ AVAILABLE_MODELS: {AVAILABLE_MODELS}')

    return {
        "OPENAI_API_KEY": OPENAI_API_KEY,
        "OPENAI_BASE_URL": OPENAI_BASE_URL,
        "MODEL_LIST": MODEL_LIST,
        "CURRENT_MODEL": CURRENT_MODEL,
        "TEMPERATURE": 0.8
    }

# ä»…é¦–æ¬¡æ‰§è¡Œ
env_vars = load_env_vars()

with st.sidebar:
    env_vars["CURRENT_MODEL"] = st.selectbox(
        label="é€‰æ‹©æ¨¡å‹",
        options=env_vars["MODEL_LIST"],
        index=0,
        help="é€‰æ‹© LLM æ¨¡å‹çš„ç§ç±»",
    )
    # æ»‘åŠ¨æ¡
    env_vars['TEMPERATURE'] = st.slider(
        label="Temperature",
        min_value=0.0, max_value=1.0, value=0.8,
        help="Temperature å‚æ•°ç”¨äºæ§åˆ¶ LLM çš„è¾“å‡ºå¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œé«˜ Temperature å¢åŠ å¤šæ ·æ€§ä½†å¯èƒ½é™ä½ç¡®å®šæ€§ï¼Œä½ Temperature åˆ™å¢åŠ ç¡®å®šæ€§ä½†å¯èƒ½é™ä½å¤šæ ·æ€§ã€‚"
    )

if "messages" not in st.session_state:
    st.session_state["messages"] = [ChatMessage(role="assistant", content="ğŸ™ƒ æœ‰é—®é¢˜å¯ä»¥å‘æˆ‘æé—®å“¦~")]

for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

if prompt := st.chat_input(placeholder = "shift+enter æ¢è¡Œ", max_chars = 2048):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        llm = ChatOpenAI(
            openai_api_key=env_vars["OPENAI_API_KEY"], 
            model_name=env_vars["CURRENT_MODEL"], 
            temperature=env_vars["TEMPERATURE"], 
            base_url=env_vars["OPENAI_BASE_URL"], 
            streaming=True, 
            callbacks=[stream_handler]
        )
        print(f"{time.strftime('[%Y-%m-%d %H:%M:%S]', time.localtime())} [{env_vars["CURRENT_MODEL"]}] {[message.content for message in st.session_state.messages]}")
        response = llm.invoke(st.session_state.messages)
        st.session_state.messages.append(ChatMessage(role="assistant", content=response.content))

